{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Определяю какие-то функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    if batch_size > X.shape[0]:\n",
    "        batch_size = X.shape[0]\n",
    "    if shuffle:\n",
    "        new_ids = np.random.permutation(X.shape[0])\n",
    "    else:\n",
    "        new_ids = np.arange(X.shape[0])\n",
    "    number_batches = X.shape[0] // batch_size\n",
    "    for i in range(number_batches):\n",
    "        indices = range(batch_size*i, batch_size*(i+1))\n",
    "        X_batch = X[new_ids[indices]]\n",
    "        y_batch = y[new_ids[indices]]\n",
    "        yield (X_batch, y_batch)\n",
    "    if len(X) % batch_size != 0:\n",
    "        indices = batch_size*(i+1)\n",
    "        X_batch = X[new_ids[indices:]]\n",
    "        y_batch = y[new_ids[indices:]]\n",
    "        yield (X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    sigm_value_x = 1/(1 + np.exp(-(x)))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):   \n",
    "    def __init__(self, batch_generator, batch_size=50, \\\n",
    "                 C=1, alpha=0.01, max_epoch=10, model_type='logreg', th=0.5):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "        \n",
    "        self.th = th\n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        self.weights = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0.\n",
    "        if self.model_type == 'linreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                a = np.dot(x, self.weights)\n",
    "                loss += (a-y)**2\n",
    "            loss /= len(y_batch)\n",
    "            loss += np.dot(self.weights[1:], self.weights[1:]) / self.C\n",
    "        elif self.model_type == 'logreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                a = sigmoid(np.dot(x, self.weights))\n",
    "                temp = a**y * (1-a)**(1-y)\n",
    "                if temp < 10**(-301):                       # наугад\n",
    "                    loss -= -1000                          # наугад\n",
    "                    continue\n",
    "                loss -= np.log2(temp)\n",
    "#                 loss -= y * np.log2(a) + (1-y) * np.log2(1-a)\n",
    "            loss /= len(y_batch)\n",
    "            loss += np.dot(self.weights[1:], self.weights[1:]) / self.C\n",
    "        return loss\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"      \n",
    "        loss_grad = 0.\n",
    "        if self.model_type == 'linreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                a = np.dot(x, self.weights)\n",
    "                loss_grad += (a-y)*x\n",
    "            loss_grad /= len(y_batch)\n",
    "            R = self.weights / self.C\n",
    "            R[0] = 0\n",
    "            loss_grad += R\n",
    "        elif self.model_type == 'logreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                dot = np.dot(x, self.weights)\n",
    "                a = sigmoid(dot)\n",
    "                loss_grad += (a-y)*x\n",
    "            loss_grad /= len(y_batch)\n",
    "            R = self.weights / self.C\n",
    "            R[0] = 0\n",
    "            loss_grad += R\n",
    "        return loss_grad\n",
    "    \n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"    \n",
    "        alpha_k = self.alpha / self.curr_epoch**(0.005)\n",
    "        self.weights = self.weights - alpha_k * new_grad\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''    \n",
    "        if self.model_type == 'linreg':\n",
    "            y = y - np.mean(y)\n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        self.weights = X[np.random.randint(0, X.shape[0]-1)]\n",
    "        self.curr_epoch = 0\n",
    "        for n in range(0, self.max_epoch):\n",
    "            self.curr_epoch += 1\n",
    "            new_epoch_generator = self.batch_generator(X, y, shuffle=True, batch_size=self.batch_size)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)  \n",
    "        return self\n",
    "     \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''        \n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        y_hat = np.array([])        \n",
    "        if self.model_type == 'linreg':\n",
    "            y_hat = np.dot(X, self.weights) / np.sum(X)\n",
    "        elif self.model_type == 'logreg':\n",
    "            dot_func = lambda x: sigmoid(np.dot(x, self.weights))\n",
    "            y_hat = np.apply_along_axis(dot_func, 1, X)\n",
    "        y_hat = np.vstack((1-y_hat, y_hat)).T\n",
    "        return y_hat\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''        \n",
    "        y_hat = self.predict_proba(X)\n",
    "        if self.model_type == 'logreg':\n",
    "            y_hat = y_hat - self.th > 0\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rforest_plus_logreg(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, SGD_mod, RForest_mod, th=0.5, balance_ratio=0.5):\n",
    "        self.SGD_mod = SGD_mod\n",
    "        self.RForest_mod = RForest_mod\n",
    "        self.th = th\n",
    "        self.balance_ratio = balance_ratio\n",
    "    def fit(self, X, y):\n",
    "        self.SGD_mod.fit(X,y)\n",
    "        self.RForest_mod.fit(X,y)\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        y_pred = self.SGD_mod.predict_proba(X)[:,1]\n",
    "        y_pred = np.vstack((y_pred, self.RForest_mod.predict_proba(X)[:,1]))\n",
    "        y_pred = (1 - self.balance_ratio) * y_pred[0] + self.balance_ratio * y_pred[1]\n",
    "#         y_pred = np.mean(y_pred, axis=0)   \n",
    "        y_pred = np.vstack((1-y_pred, y_pred)).T\n",
    "        return y_pred\n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_proba(X)[:,1]\n",
    "        return (y_pred - self.th > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_generator(groups_train, n_splits=10):\n",
    "    all_groups = np.unique(groups_train)\n",
    "    fold_size = len(all_groups) // n_splits\n",
    "    all_groups = np.random.permutation(all_groups)\n",
    "    fold_groups = np.zeros((n_splits,fold_size), dtype=int)\n",
    "    for i, group in enumerate(all_groups):\n",
    "        fold = i // fold_size\n",
    "        if fold == n_splits:\n",
    "            break\n",
    "        group_i = i % fold_size\n",
    "        fold_groups[fold,group_i] = group\n",
    "    fold_indices = {}\n",
    "    for fold in range(n_splits):\n",
    "        indices = np.array([], dtype = int)\n",
    "        for group in fold_groups[fold]:\n",
    "            indices = np.append(indices, np.argwhere(groups_train == group))\n",
    "        fold_indices[fold] = indices\n",
    "\n",
    "    for i in fold_indices:\n",
    "        kf_test = fold_indices[i]\n",
    "        kf_train = np.array([],dtype=int)\n",
    "        for j in fold_indices:\n",
    "            if i == j:\n",
    "                continue\n",
    "            kf_train = np.append(kf_train, fold_indices[j])\n",
    "        kf_tuple = [kf_train, kf_test]\n",
    "        yield (kf_train, kf_test)\n",
    "        \n",
    "def cross_validation(model, groups_train, kfold_generator, X, y, \\\n",
    "                     folds=10, th=0.5, verbose=False):    \n",
    "    total_score = 0.\n",
    "    total_ac_score = 0.\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_test)[:,1]  \n",
    "            \n",
    "        score = f1_score(y_test, (y_pred - th > 0))\n",
    "        ac_score = accuracy_score(y_test, (y_pred - th > 0))\n",
    "        total_score += score\n",
    "        total_ac_score += ac_score\n",
    "        if verbose:\n",
    "            print(i, \"score:\", score)\n",
    "    mean_score = total_score / folds\n",
    "    mean_ac_score = total_ac_score / folds\n",
    "    if verbose:\n",
    "        print(\"MEAN_SCORE:\", mean_score)\n",
    "    return mean_score, mean_ac_score\n",
    "\n",
    "def grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "            X, y, groups_train, kfold_generator, batch_generator, \\\n",
    "            model_type='logreg', folds=10, repeats=1, verbose=True):\n",
    "    sample_scores = np.array([])\n",
    "    sample_ac_scores = np.array([])\n",
    "    sample_params = []\n",
    "    for alpha in alpha_list:\n",
    "        for C in C_list:\n",
    "            for max_epoch in max_epoch_list:\n",
    "                for th in th_list: \n",
    "                    for max_features in max_features_list: \n",
    "                        X_curr = X[:,:max_features]\n",
    "                        curr_mean_score_list = np.array([])\n",
    "                        curr_mean_ac_score_list = np.array([])\n",
    "                        for r in range(repeats):\n",
    "                            if model_type == 'rforest_logreg' or model_type == 'logreg_rforest':\n",
    "                                RForest_mod = RandomForestClassifier(max_depth=8, min_samples_split=10, \\\n",
    "                                                                     n_estimators=20, min_samples_leaf=5, \\\n",
    "                                                                     max_features=7, criterion='entropy')\n",
    "                                SGD_mod = MySGDClassifier(batch_generator=batch_generator, \\\n",
    "                                                          model_type='logreg', \\\n",
    "                                                          alpha=alpha, C=C, max_epoch=max_epoch)\n",
    "                                model = rforest_plus_logreg(SGD_mod, RForest_mod, th=th)\n",
    "                            elif model_type == 'logreg' or model_type == 'linreg':\n",
    "                                model = MySGDClassifier(batch_generator=batch_generator, \\\n",
    "                                                        model_type=model_type, \\\n",
    "                                                        alpha=alpha, C=C, max_epoch=max_epoch, th=th)\n",
    "                            elif model_type == 'rforest':\n",
    "                                model = RandomForestClassifier(max_depth=8, min_samples_split=10, \\\n",
    "                                                                     n_estimators=20, min_samples_leaf=5, \\\n",
    "                                                                     max_features=7, criterion='entropy')\n",
    "\n",
    "                            curr_score, curr_ac_score = cross_validation(model, groups_train, \\\n",
    "                                                                         kfold_generator, X_curr, y, \\\n",
    "                                                                         folds=folds, th=th)\n",
    "                            curr_mean_score_list = np.append(curr_mean_score_list, curr_score)\n",
    "                            curr_mean_ac_score_list = np.append(curr_mean_ac_score_list, curr_ac_score)\n",
    "                        curr_mean_score = curr_mean_score_list.mean()\n",
    "                        curr_mean_ac_score = curr_mean_ac_score_list.mean()\n",
    "                        sample_scores = np.append(sample_scores, curr_mean_score)\n",
    "                        sample_ac_scores = np.append(sample_ac_scores, curr_mean_ac_score)\n",
    "                        sample_tuple = (alpha, C, th, max_epoch, max_features)\n",
    "                        sample_params.append(sample_tuple)\n",
    "                        if verbose:\n",
    "                            print(\"SCORE: %.5f\" % curr_mean_score, end='\\t')\n",
    "                            print(\"ACC: %.3f\" % curr_mean_ac_score, end='\\t')\n",
    "                            print(\"(alpha = %s; C = %s; max_epoch = %s; th = %s; max_features = %s)\" \\\n",
    "                                  % (alpha, C, max_epoch, th, max_features))\n",
    "    best_score_index = np.argmax(sample_scores)\n",
    "    best_score = sample_scores[best_score_index]\n",
    "    best_params = sample_params[best_score_index]\n",
    "    if verbose:\n",
    "        print(\"\\nBEST SCORE:\\t\", best_score)\n",
    "        print(\"BEST PARAMS:\\t\", best_params)\n",
    "    return best_score, best_params, sample_scores, sample_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(useful_words_tsv, min_length=0):\n",
    "    doc_to_title = {}\n",
    "    with open(useful_words_tsv) as f:\n",
    "        for num_line, line in enumerate(f):\n",
    "            if num_line == 0:\n",
    "                continue\n",
    "            data = line.strip().split('\\t', 1)\n",
    "            doc_id = int(data[0])\n",
    "            if len(data) == 1:\n",
    "                title = ''\n",
    "            else:\n",
    "                title = data[1]\n",
    "#           магические 5 строчек!---------\n",
    "            cur = re.split(r' ',title)\n",
    "            title = ''\n",
    "            for i in cur:\n",
    "                if len(i) >= min_length:\n",
    "                    title += i + ' '\n",
    "#           ------------------------------          \n",
    "            doc_to_title[doc_id] = title\n",
    "    return doc_to_title\n",
    "\n",
    "def preprocessing_2(train_or_test_groups_csv, doc_to_title, train=True):\n",
    "    train_data = pd.read_csv(train_or_test_groups_csv)\n",
    "    traingroups_titledata = {}\n",
    "    for i in range(len(train_data)):\n",
    "        new_doc = train_data.iloc[i]\n",
    "        doc_group = new_doc['group_id']\n",
    "        doc_id = new_doc['doc_id']\n",
    "        title = doc_to_title[doc_id]\n",
    "        if doc_group not in traingroups_titledata:\n",
    "            traingroups_titledata[doc_group] = []\n",
    "        if train:\n",
    "            target = new_doc['target']\n",
    "            traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "        else:\n",
    "            traingroups_titledata[doc_group].append((doc_id, title))\n",
    "    return traingroups_titledata\n",
    "\n",
    "def preprocessing_3_old(traingroups_titledata, num_features=15, train=True):\n",
    "    y_train = []\n",
    "    X_train = []\n",
    "    groups_train = []\n",
    "    for new_group in traingroups_titledata:\n",
    "        docs = traingroups_titledata[new_group] \n",
    "        for k, tup in enumerate(docs):\n",
    "            if train:\n",
    "                doc_id, title, target_id = tup\n",
    "                y_train.append(target_id)\n",
    "            else:\n",
    "                doc_id, title = tup\n",
    "            groups_train.append(new_group)\n",
    "            all_dist = []\n",
    "            words = set(title.strip().split())\n",
    "            for j in range(0, len(docs)):\n",
    "                if k == j:\n",
    "                    continue\n",
    "                if train:\n",
    "                    doc_id_j, title_j, target_j = docs[j]\n",
    "                else:\n",
    "                    doc_id_j, title_j = docs[j]\n",
    "                words_j = set(title_j.strip().split())\n",
    "                all_dist.append(len(words.intersection(words_j)))\n",
    "            X_train.append(sorted(all_dist, reverse=True)[0:num_features])\n",
    "    if train:\n",
    "        return np.array(X_train), np.array(y_train), np.array(groups_train)\n",
    "    else:\n",
    "        return np.array(X_train), np.array([]), np.array(groups_train)\n",
    "    \n",
    "def preprocessing_3(traingroups_titledata, num_features=15, num_tfidf_features=30, train=True):\n",
    "    y = []\n",
    "    X = []\n",
    "    groups = []\n",
    "    for new_group in traingroups_titledata:\n",
    "        docs = traingroups_titledata[new_group] \n",
    "        list_data = []\n",
    "        for k, tup in enumerate(docs):\n",
    "            if train:\n",
    "                doc_id, title, target_id = tup\n",
    "                y.append(target_id)\n",
    "            else:\n",
    "                doc_id, title = tup\n",
    "            list_data.append(title)  \n",
    "            groups.append(new_group)\n",
    "\n",
    "        vectorizer = TfidfVectorizer(max_features=num_tfidf_features)\n",
    "        group_voc = vectorizer.fit_transform(list_data)\n",
    "        dist = cosine_similarity(group_voc, group_voc)\n",
    "        X_curr  = np.sort(dist, axis=1)[:,-(num_features+1):-1][:,::-1]\n",
    "        X.append(X_curr)\n",
    "    X = np.vstack(X)\n",
    "    if train:\n",
    "        return np.array(X), np.array(y), np.array(groups)\n",
    "    else:\n",
    "        return np.array(X), np.array(groups)\n",
    "    \n",
    "def preprocessing(useful_words_tsv, train_or_test_groups_csv, min_length, num_features, num_tfidf_features, \n",
    "                  train=True):\n",
    "    doc_to_title = preprocessing_1(useful_words_tsv, min_length=min_length)\n",
    "    traingroups_titledata = preprocessing_2(train_or_test_groups_csv, doc_to_title, train=train)\n",
    "    tup = preprocessing_3(traingroups_titledata, num_features=num_features, \\\n",
    "                                    num_tfidf_features=num_tfidf_features, train=train)\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cross_validation_old(alpha, C, max_epoch, th, \\\n",
    "                           model_list, X_list, y, \\\n",
    "                           groups_train, kfold_generator, folds=10, verbose=False):    \n",
    "    total_score = 0.\n",
    "    total_ac_score = 0.\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        first_train_index, first_test_index, second_train_index, second_test_index = \\\n",
    "            train_index[0::2], test_index[0::2], \\\n",
    "            train_index[1::2], test_index[1::2]\n",
    "        \n",
    "        y_train, y_test = y[first_train_index], y[first_test_index]\n",
    "        y_pred_model = []\n",
    "        for model, X in zip(model_list, X_list):\n",
    "            X_train, X_test = X[first_train_index], X[first_test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_model.append(model.predict_proba(X_test)[:,1])\n",
    "        \n",
    "        X_train = np.vstack(y_pred_model).T       \n",
    "        total_model = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                          alpha=alpha, C=C, max_epoch=max_epoch, th=th)\n",
    "        total_model.fit(X_train, y_test)\n",
    "        \n",
    "        y_train, y_test = y[second_train_index], y[second_test_index]\n",
    "        y_pred_model = []\n",
    "        for model, X in zip(model_list, X_list):\n",
    "            X_train, X_test = X[second_train_index], X[second_test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_model.append(model.predict_proba(X_test)[:,1])\n",
    "\n",
    "        X_test = np.vstack(y_pred_model).T       \n",
    "        y_pred = total_model.predict(X_test)[:,1]\n",
    "        \n",
    "        score = f1_score(y_test, y_pred)\n",
    "        ac_score = accuracy_score(y_test, y_pred)\n",
    "        total_score += score\n",
    "        total_ac_score += ac_score\n",
    "        if verbose:\n",
    "            print(i, \"score:\", score)\n",
    "    mean_score = total_score / folds\n",
    "    mean_ac_score = total_ac_score / folds\n",
    "    if verbose:\n",
    "        print(\"MEAN_SCORE:\", mean_score)\n",
    "    return mean_score, mean_ac_score\n",
    "\n",
    "def total_grid_cv(alpha_list, C_list, max_epoch_list, th_list, \\\n",
    "                  model_list, X_list, y, \\\n",
    "                  groups_train, kfold_generator, batch_generator, \\\n",
    "                  folds=10, repeats=1, verbose=True):\n",
    "    sample_scores = np.array([])\n",
    "    sample_ac_scores = np.array([])\n",
    "    sample_params = []\n",
    "    for alpha in alpha_list:\n",
    "        for C in C_list:\n",
    "            for max_epoch in max_epoch_list:\n",
    "                for th in th_list: \n",
    "                    curr_mean_score_list = np.array([])\n",
    "                    curr_mean_ac_score_list = np.array([])\n",
    "                    for r in range(repeats):\n",
    "                        curr_score, curr_ac_score = total_cross_validation(alpha, C, max_epoch, th, \\\n",
    "                                                      model_list, X_list, y, \\\n",
    "                                                      groups_train, kfold_generator, folds=folds)\n",
    "                        curr_mean_score_list = np.append(curr_mean_score_list, curr_score)\n",
    "                        curr_mean_ac_score_list = np.append(curr_mean_ac_score_list, curr_ac_score)\n",
    "                    curr_mean_score = curr_mean_score_list.mean()\n",
    "                    curr_mean_ac_score = curr_mean_ac_score_list.mean()\n",
    "                    sample_scores = np.append(sample_scores, curr_mean_score)\n",
    "                    sample_ac_scores = np.append(sample_ac_scores, curr_mean_ac_score)\n",
    "                    sample_tuple = (alpha, C, max_epoch, th)\n",
    "                    sample_params.append(sample_tuple)\n",
    "                    if verbose:\n",
    "                        print(\"SCORE: %.5f\" % curr_mean_score, end='\\t')\n",
    "                        print(\"ACC: %.3f\" % curr_mean_ac_score, end='\\t')\n",
    "                        print(\"(alpha = %s; C = %s; max_epoch = %s; th = %s)\" % (alpha, C, max_epoch, th))\n",
    "    best_score_index = np.argmax(sample_scores)\n",
    "    best_score = sample_scores[best_score_index]\n",
    "    best_ac_score = sample_ac_scores[best_score_index]\n",
    "    best_params = sample_params[best_score_index]\n",
    "    if verbose:\n",
    "        print(\"\\nBEST SCORE:\\t\", best_score)\n",
    "        print(\"BEST PARAMS:\\t\", best_params)\n",
    "    return best_score, best_params, sample_scores, sample_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class total_model_class(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model_list, alpha=0.1, C=100, max_epoch=10, th=0.5):\n",
    "        self.model_list = model_list\n",
    "        self.th = th\n",
    "        self.weights = []\n",
    "        self.total_model = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                                                         alpha=alpha, C=C, max_epoch=max_epoch)\n",
    "    def fit(self, X_first, y_first):\n",
    "        X_train_list, X_test_list = X_first\n",
    "        y_train, y_test = y_first\n",
    "        y_pred_model = []\n",
    "        for model, X_train, X_test in zip(self.model_list, X_train_list, X_test_list):\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_model.append(model.predict_proba(X_test)[:,1])   \n",
    "        X_train_total = np.vstack(y_pred_model).T \n",
    "    \n",
    "        self.total_model.fit(X_train_total, y_test)\n",
    "        self.weights = self.total_model.weights\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X_second, y_second):\n",
    "        X_train_list, X_test_list = X_second\n",
    "        y_train = y_second\n",
    "        y_pred_model = []\n",
    "        for model, X_train, X_test in zip(self.model_list, X_train_list, X_test_list):\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_model.append(model.predict_proba(X_test)[:,1])\n",
    "        X_test_total = np.vstack(y_pred_model).T  \n",
    "\n",
    "        y_pred = self.total_model.predict_proba(X_test_total)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X_second, y_second):\n",
    "        y_pred = self.predict_proba(X_second, y_second)[:,1]\n",
    "        return (y_pred - self.th > 0).astype(int)\n",
    "\n",
    "def total_cross_validation(alpha, C, max_epoch, th, \\\n",
    "                           model_list, X_list, y, \\\n",
    "                           groups_train, kfold_generator, folds=10, verbose=False):    \n",
    "    total_score = 0.\n",
    "    total_ac_score = 0.\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        first_train_index, first_test_index, second_train_index, second_test_index = \\\n",
    "            train_index[0::2], test_index[0::2], \\\n",
    "            train_index[1::2], test_index[1::2]\n",
    "        \n",
    "        X_first_tr_list = []\n",
    "        X_first_tst_list = []\n",
    "        for X in X_list:\n",
    "            X_first_tr_list.append(X[first_train_index])\n",
    "            X_first_tst_list.append(X[first_test_index])\n",
    "        y_first = y[first_train_index], y[first_test_index]\n",
    "        X_first = X_first_tr_list, X_first_tst_list\n",
    "            \n",
    "        X_second_tr_list = []\n",
    "        X_second_tst_list = []\n",
    "        for X in X_list:\n",
    "            X_second_tr_list.append(X[second_train_index])\n",
    "            X_second_tst_list.append(X[second_test_index])\n",
    "        y_second = y[second_train_index]\n",
    "        X_second = X_second_tr_list, X_second_tst_list\n",
    "\n",
    "\n",
    "        total_model = total_model_class(model_list, alpha=alpha, C=C, max_epoch=max_epoch, th=th)\n",
    "        total_model.fit(X_first, y_first)\n",
    "        y_pred = total_model.predict(X_second, y_second)\n",
    "        \n",
    "        score = f1_score(y[second_test_index], y_pred)\n",
    "        ac_score = accuracy_score(y[second_test_index], y_pred)\n",
    "        total_score += score\n",
    "        total_ac_score += ac_score\n",
    "        if verbose:\n",
    "            print(i, \"score:\", score)\n",
    "    mean_score = total_score / folds\n",
    "    mean_ac_score = total_ac_score / folds\n",
    "    if verbose:\n",
    "        print(\"MEAN_SCORE:\", mean_score)\n",
    "    return mean_score, mean_ac_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fit мелких моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) title_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 100000\n",
    "\n",
    "X_tr_title, y_tr_title, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_title)\n",
    "X_tr_title_sc = scaler.transform(X_tr_title)\n",
    "print(X_tr_title_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.69173\tACC: 0.806\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68302\tACC: 0.806\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68349\tACC: 0.801\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 15)\n",
      "\n",
      "BEST SCORE:\t 0.6917299594885967\n",
      "BEST PARAMS:\t (0.1, 500, 0.27, 8, 25)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.1]\n",
    "C_list = [500]\n",
    "max_epoch_list = [8]\n",
    "th_list = [0.27]\n",
    "max_features_list = [25, 20, 15]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "                                    X_tr_title_sc, y_tr_title, groups_train, kfold_generator, batch_generator, \\\n",
    "                                    model_type='logreg', folds=6, repeats=6, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 100000\n",
    "\n",
    "\n",
    "X_tr_title, y_tr_title, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features) \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_title)\n",
    "X_tr_title_sc = scaler.transform(X_tr_title)\n",
    "print(X_tr_title_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySGDClassifier(C=500, alpha=0.1,\n",
       "                batch_generator=<function batch_generator at 0x7fd89abb59d8>,\n",
       "                batch_size=50, max_epoch=8, model_type='logreg', th=0.27)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "C = 500\n",
    "max_epoch = 8\n",
    "\n",
    "title_model = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                            alpha=alpha, C=C, max_epoch=max_epoch) \n",
    "title_model.fit(X_tr_title_sc, y_tr_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_pred = h1_model.predict_proba(X_tr_title_sc)[:,1]\n",
    "y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) h1_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 100000\n",
    "\n",
    "X_tr_h1, y_tr_h1, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features) \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_h1)\n",
    "X_tr_h1_sc = scaler.transform(X_tr_h1)\n",
    "print(X_tr_h1_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.68499\tACC: 0.807\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.67993\tACC: 0.808\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68027\tACC: 0.805\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 15)\n",
      "SCORE: 0.67516\tACC: 0.801\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 10)\n",
      "SCORE: 0.67074\tACC: 0.795\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 5)\n",
      "\n",
      "BEST SCORE:\t 0.6849914256458396\n",
      "BEST PARAMS:\t (0.1, 500, 0.27, 8, 25)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.1]\n",
    "C_list = [500]\n",
    "max_epoch_list = [8]\n",
    "th_list = [0.27]\n",
    "max_features_list = [25,20,15,10,5]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "                                    X_tr_h1_sc, y_tr_h1, groups_train, kfold_generator, batch_generator, \\\n",
    "                                    model_type='logreg', folds=6, repeats=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 100000\n",
    "\n",
    "X_tr_h1, y_tr_h1, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features = num_tfidf_features) \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_h1)\n",
    "X_tr_h1_sc = scaler.transform(X_tr_h1)\n",
    "print(X_tr_h1_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySGDClassifier(C=500, alpha=0.1,\n",
       "                batch_generator=<function batch_generator at 0x7fd89abb59d8>,\n",
       "                batch_size=50, max_epoch=8, model_type='logreg', th=0.5)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "C = 500\n",
    "max_epoch = 8\n",
    "\n",
    "h1_model = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                            alpha=alpha, C=C, max_epoch=max_epoch) \n",
    "h1_model.fit(X_tr_h1_sc, y_tr_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = h1_model.predict_proba(X_tr_h1_sc)[:,1]\n",
    "y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## с) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fit общей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [title_model, h1_model]\n",
    "X_list = [X_tr_h1_sc, X_tr_title_sc]\n",
    "y_train = y_tr_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.70591\tACC: 0.813\t(alpha = 0.15; C = 500; max_epoch = 10; th = 0.27)\n",
      "SCORE: 0.70710\tACC: 0.818\t(alpha = 0.15; C = 500; max_epoch = 10; th = 0.28)\n",
      "SCORE: 0.68521\tACC: 0.817\t(alpha = 0.15; C = 500; max_epoch = 10; th = 0.29)\n",
      "SCORE: 0.70672\tACC: 0.824\t(alpha = 0.15; C = 500; max_epoch = 30; th = 0.27)\n",
      "SCORE: 0.70680\tACC: 0.823\t(alpha = 0.15; C = 500; max_epoch = 30; th = 0.28)\n",
      "SCORE: 0.69715\tACC: 0.821\t(alpha = 0.15; C = 500; max_epoch = 30; th = 0.29)\n",
      "\n",
      "BEST SCORE:\t 0.7070969040158275\n",
      "BEST PARAMS:\t (0.15, 500, 10, 0.28)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.15]\n",
    "C_list = [500]\n",
    "max_epoch_list = [10, 30]\n",
    "th_list = [0.27, 0.28, 0.29]\n",
    "\n",
    "best_score, _, sample_scores, _ = total_grid_cv(alpha_list, C_list, max_epoch_list, th_list, \\\n",
    "                                    model_list, X_list, y_train, \\\n",
    "                                    groups_train, kfold_generator, batch_generator, \\\n",
    "                                    folds=3, repeats=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.15\n",
    "C = 500\n",
    "max_epoch = 30\n",
    "th = 0.27\n",
    "\n",
    "total_model = total_model_class(model_list, alpha=alpha, C=C, max_epoch=max_epoch, th=th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.69508415,  2.54027195,  3.01626598])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeats = 10\n",
    "folds = 3\n",
    "weights_list = []\n",
    "\n",
    "for rep in range(repeats):\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        X_first_tr_list = []\n",
    "        X_first_tst_list = []\n",
    "        for X in X_list:\n",
    "            X_first_tr_list.append(X[train_index])\n",
    "            X_first_tst_list.append(X[test_index])\n",
    "        y_first = y[train_index], y[test_index]\n",
    "        X_first = X_first_tr_list, X_first_tst_list\n",
    "\n",
    "        total_model = total_model_class(model_list, alpha=alpha, C=C, max_epoch=max_epoch, th=th)\n",
    "        total_model.fit(X_first, y_first)\n",
    "\n",
    "        weights_list.append(total_model.weights)\n",
    "weights = np.vstack(weights_list).mean(axis=0)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6994535519125683, 0.8306244653550042)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = do_prediction(weights, y_pred_list, th=0.27).astype(int)\n",
    "f1_score(pr, y_train), \\\n",
    "accuracy_score(pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.69508415,  2.54027195,  3.01626598])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.43584684,  0.6779059 ,  0.78940243])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, test_groups_csv, out_file, target='target', index_label=\"pair_id\"):\n",
    "    indices = np.asarray(pd.read_csv(test_groups_csv)[index_label])\n",
    "    predicted_df = pd.DataFrame(predicted_labels, index = indices, columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(weights, y_pred_list, th=0.5):\n",
    "    X = np.vstack(y_pred_list)\n",
    "    X = np.vstack((np.ones(X.shape[1]), X)).T\n",
    "    \n",
    "    dot_func = lambda x: sigmoid(np.dot(x, weights))\n",
    "    y_pred = np.apply_along_axis(dot_func, 1, X)\n",
    "    return (y_pred - th > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка тестовых фичей для total_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) X_tst_title_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 25) 1\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 100000\n",
    "\n",
    "X_tst_title, groups_test = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features, train=False) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tst_title)\n",
    "X_tst_title_sc = scaler.transform(X_tst_title)\n",
    "y_pred = title_model.predict_proba(X_tst_title_sc)[:,1]\n",
    "y_pred_list.append(y_pred)\n",
    "\n",
    "print(X_tst_title_sc.shape, len(y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) X_tst_h1_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 25) 2\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 100000\n",
    "\n",
    "X_tst_h1, groups_test = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features, train=False) \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tst_h1)\n",
    "X_tst_h1_sc = scaler.transform(X_tst_h1)\n",
    "y_pred = h1_model.predict_proba(X_tst_h1_sc)[:,1]\n",
    "y_pred_list.append(y_pred)\n",
    "\n",
    "print(X_tst_h1_sc.shape, len(y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = do_prediction(weights, y_pred_list, th=0.28)\n",
    "write_to_submission_file(y_pred, 'test_groups.csv', \"y_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
