{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Определяю какие-то функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    if batch_size > X.shape[0]:\n",
    "        batch_size = X.shape[0]\n",
    "    if shuffle:\n",
    "        new_ids = np.random.permutation(X.shape[0])\n",
    "    else:\n",
    "        new_ids = np.arange(X.shape[0])\n",
    "    number_batches = X.shape[0] // batch_size\n",
    "    for i in range(number_batches):\n",
    "        indices = range(batch_size*i, batch_size*(i+1))\n",
    "        X_batch = X[new_ids[indices]]\n",
    "        y_batch = y[new_ids[indices]]\n",
    "        yield (X_batch, y_batch)\n",
    "    if len(X) % batch_size != 0:\n",
    "        indices = batch_size*(i+1)\n",
    "        X_batch = X[new_ids[indices:]]\n",
    "        y_batch = y[new_ids[indices:]]\n",
    "        yield (X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    sigm_value_x = 1/(1 + np.exp(-(x)))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):   \n",
    "    def __init__(self, batch_generator, batch_size=50, \\\n",
    "                 C=1, alpha=0.01, max_epoch=10, model_type='logreg', th=0.5):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "        \n",
    "        self.th = th\n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        self.weights = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0.\n",
    "        if self.model_type == 'linreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                a = np.dot(x, self.weights)\n",
    "                loss += (a-y)**2\n",
    "            loss /= len(y_batch)\n",
    "            loss += np.dot(self.weights[1:], self.weights[1:]) / self.C\n",
    "        elif self.model_type == 'logreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                a = sigmoid(np.dot(x, self.weights))\n",
    "                temp = a**y * (1-a)**(1-y)\n",
    "                if temp < 10**(-301):                       # наугад\n",
    "                    loss -= -1000                          # наугад\n",
    "                    continue\n",
    "                loss -= np.log2(temp)\n",
    "#                 loss -= y * np.log2(a) + (1-y) * np.log2(1-a)\n",
    "            loss /= len(y_batch)\n",
    "            loss += np.dot(self.weights[1:], self.weights[1:]) / self.C\n",
    "        return loss\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"      \n",
    "        loss_grad = 0.\n",
    "        if self.model_type == 'linreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                a = np.dot(x, self.weights)\n",
    "                loss_grad += (a-y)*x\n",
    "            loss_grad /= len(y_batch)\n",
    "            R = self.weights / self.C\n",
    "            R[0] = 0\n",
    "            loss_grad += R\n",
    "        elif self.model_type == 'logreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                dot = np.dot(x, self.weights)\n",
    "                a = sigmoid(dot)\n",
    "                loss_grad += (a-y)*x\n",
    "            loss_grad /= len(y_batch)\n",
    "            R = self.weights / self.C\n",
    "            R[0] = 0\n",
    "            loss_grad += R\n",
    "        return loss_grad\n",
    "    \n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"    \n",
    "        alpha_k = self.alpha / self.curr_epoch**(0.005)\n",
    "        self.weights = self.weights - alpha_k * new_grad\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''    \n",
    "        if self.model_type == 'linreg':\n",
    "            y = y - np.mean(y)\n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        self.weights = X[np.random.randint(0, X.shape[0]-1)]\n",
    "        self.curr_epoch = 0\n",
    "        for n in range(0, self.max_epoch):\n",
    "            self.curr_epoch += 1\n",
    "            new_epoch_generator = self.batch_generator(X, y, shuffle=True, batch_size=self.batch_size)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)  \n",
    "        return self\n",
    "     \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''        \n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        y_hat = np.array([])        \n",
    "        if self.model_type == 'linreg':\n",
    "            y_hat = np.dot(X, self.weights) / np.sum(X)\n",
    "        elif self.model_type == 'logreg':\n",
    "            dot_func = lambda x: sigmoid(np.dot(x, self.weights))\n",
    "            y_hat = np.apply_along_axis(dot_func, 1, X)\n",
    "        y_hat = np.vstack((1-y_hat, y_hat)).T\n",
    "        return y_hat\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''        \n",
    "        y_hat = self.predict_proba(X)\n",
    "        if self.model_type == 'logreg':\n",
    "            y_hat = y_hat - self.th > 0\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rforest_plus_logreg(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, SGD_mod, RForest_mod, th=0.5, balance_ratio=0.5):\n",
    "        self.SGD_mod = SGD_mod\n",
    "        self.RForest_mod = RForest_mod\n",
    "        self.th = th\n",
    "        self.balance_ratio = balance_ratio\n",
    "    def fit(self, X, y):\n",
    "        self.SGD_mod.fit(X,y)\n",
    "        self.RForest_mod.fit(X,y)\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        y_pred = self.SGD_mod.predict_proba(X)[:,1]\n",
    "        y_pred = np.vstack((y_pred, self.RForest_mod.predict_proba(X)[:,1]))\n",
    "        y_pred = (1 - self.balance_ratio) * y_pred[0] + self.balance_ratio * y_pred[1]\n",
    "#         y_pred = np.mean(y_pred, axis=0)   \n",
    "        y_pred = np.vstack((1-y_pred, y_pred)).T\n",
    "        return y_pred\n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_proba(X)[:,1]\n",
    "        return (y_pred - self.th > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_generator(groups_train, n_splits=10):\n",
    "    all_groups = np.unique(groups_train)\n",
    "    fold_size = len(all_groups) // n_splits\n",
    "    all_groups = np.random.permutation(all_groups)\n",
    "    fold_groups = np.zeros((n_splits,fold_size), dtype=int)\n",
    "    for i, group in enumerate(all_groups):\n",
    "        fold = i // fold_size\n",
    "        if fold == n_splits:\n",
    "            break\n",
    "        group_i = i % fold_size\n",
    "        fold_groups[fold,group_i] = group\n",
    "    fold_indices = {}\n",
    "    for fold in range(n_splits):\n",
    "        indices = np.array([], dtype = int)\n",
    "        for group in fold_groups[fold]:\n",
    "            indices = np.append(indices, np.argwhere(groups_train == group))\n",
    "        fold_indices[fold] = indices\n",
    "\n",
    "    for i in fold_indices:\n",
    "        kf_test = fold_indices[i]\n",
    "        kf_train = np.array([],dtype=int)\n",
    "        for j in fold_indices:\n",
    "            if i == j:\n",
    "                continue\n",
    "            kf_train = np.append(kf_train, fold_indices[j])\n",
    "        kf_tuple = [kf_train, kf_test]\n",
    "        yield (kf_train, kf_test)\n",
    "        \n",
    "def cross_validation(model, groups_train, kfold_generator, X, y, \\\n",
    "                     folds=10, th=0.5, verbose=False):    \n",
    "    total_score = 0.\n",
    "    total_ac_score = 0.\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_test)[:,1]  \n",
    "            \n",
    "        score = f1_score(y_test, (y_pred - th > 0))\n",
    "        ac_score = accuracy_score(y_test, (y_pred - th > 0))\n",
    "        total_score += score\n",
    "        total_ac_score += ac_score\n",
    "        if verbose:\n",
    "            print(i, \"score:\", score)\n",
    "    mean_score = total_score / folds\n",
    "    mean_ac_score = total_ac_score / folds\n",
    "    if verbose:\n",
    "        print(\"MEAN_SCORE:\", mean_score)\n",
    "    return mean_score, mean_ac_score\n",
    "\n",
    "def grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "            X, y, groups_train, kfold_generator, batch_generator, \\\n",
    "            model_type='logreg', folds=10, repeats=1, verbose=True):\n",
    "    sample_scores = np.array([])\n",
    "    sample_ac_scores = np.array([])\n",
    "    sample_params = []\n",
    "    for alpha in alpha_list:\n",
    "        for C in C_list:\n",
    "            for max_epoch in max_epoch_list:\n",
    "                for th in th_list: \n",
    "                    for max_features in max_features_list: \n",
    "                        X_curr = X[:,:max_features]\n",
    "                        curr_mean_score_list = np.array([])\n",
    "                        curr_mean_ac_score_list = np.array([])\n",
    "                        for r in range(repeats):\n",
    "                            if model_type == 'rforest_logreg' or model_type == 'logreg_rforest':\n",
    "                                RForest_mod = RandomForestClassifier(max_depth=8, min_samples_split=10, \\\n",
    "                                                                     n_estimators=20, min_samples_leaf=5, \\\n",
    "                                                                     max_features=7, criterion='entropy')\n",
    "                                SGD_mod = MySGDClassifier(batch_generator=batch_generator, \\\n",
    "                                                          model_type='logreg', \\\n",
    "                                                          alpha=alpha, C=C, max_epoch=max_epoch)\n",
    "                                model = rforest_plus_logreg(SGD_mod, RForest_mod, th=th)\n",
    "                            elif model_type == 'logreg' or model_type == 'linreg':\n",
    "                                model = MySGDClassifier(batch_generator=batch_generator, \\\n",
    "                                                        model_type=model_type, \\\n",
    "                                                        alpha=alpha, C=C, max_epoch=max_epoch, th=th)\n",
    "                            elif model_type == 'rforest':\n",
    "                                model = RandomForestClassifier(max_depth=8, min_samples_split=10, \\\n",
    "                                                                     n_estimators=20, min_samples_leaf=5, \\\n",
    "                                                                     max_features=7, criterion='entropy')\n",
    "\n",
    "                            curr_score, curr_ac_score = cross_validation(model, groups_train, \\\n",
    "                                                                         kfold_generator, X_curr, y, \\\n",
    "                                                                         folds=folds, th=th)\n",
    "                            curr_mean_score_list = np.append(curr_mean_score_list, curr_score)\n",
    "                            curr_mean_ac_score_list = np.append(curr_mean_ac_score_list, curr_ac_score)\n",
    "                        curr_mean_score = curr_mean_score_list.mean()\n",
    "                        curr_mean_ac_score = curr_mean_ac_score_list.mean()\n",
    "                        sample_scores = np.append(sample_scores, curr_mean_score)\n",
    "                        sample_ac_scores = np.append(sample_ac_scores, curr_mean_ac_score)\n",
    "                        sample_tuple = (alpha, C, th, max_epoch, max_features)\n",
    "                        sample_params.append(sample_tuple)\n",
    "                        if verbose:\n",
    "                            print(\"SCORE: %.5f\" % curr_mean_score, end='\\t')\n",
    "                            print(\"ACC: %.3f\" % curr_mean_ac_score, end='\\t')\n",
    "                            print(\"(alpha = %s; C = %s; max_epoch = %s; th = %s; max_features = %s)\" \\\n",
    "                                  % (alpha, C, max_epoch, th, max_features))\n",
    "    best_score_index = np.argmax(sample_scores)\n",
    "    best_score = sample_scores[best_score_index]\n",
    "    best_params = sample_params[best_score_index]\n",
    "    if verbose:\n",
    "        print(\"\\nBEST SCORE:\\t\", best_score)\n",
    "        print(\"BEST PARAMS:\\t\", best_params)\n",
    "    return best_score, best_params, sample_scores, sample_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(useful_words_tsv, min_length=0):\n",
    "    doc_to_title = {}\n",
    "    with open(useful_words_tsv) as f:\n",
    "        for num_line, line in enumerate(f):\n",
    "            if num_line == 0:\n",
    "                continue\n",
    "            data = line.strip().split('\\t', 1)\n",
    "            doc_id = int(data[0])\n",
    "            if len(data) == 1:\n",
    "                title = ''\n",
    "            else:\n",
    "                title = data[1]\n",
    "#           магические 5 строчек!---------\n",
    "            cur = re.split(r' ',title)\n",
    "            title = ''\n",
    "            for i in cur:\n",
    "                if len(i) >= min_length:\n",
    "                    title += i + ' '\n",
    "#           ------------------------------          \n",
    "            doc_to_title[doc_id] = title\n",
    "    return doc_to_title\n",
    "\n",
    "def preprocessing_2(train_or_test_groups_csv, doc_to_title, train=True):\n",
    "    train_data = pd.read_csv(train_or_test_groups_csv)\n",
    "    traingroups_titledata = {}\n",
    "    for i in range(len(train_data)):\n",
    "        new_doc = train_data.iloc[i]\n",
    "        doc_group = new_doc['group_id']\n",
    "        doc_id = new_doc['doc_id']\n",
    "        title = doc_to_title[doc_id]\n",
    "        if doc_group not in traingroups_titledata:\n",
    "            traingroups_titledata[doc_group] = []\n",
    "        if train:\n",
    "            target = new_doc['target']\n",
    "            traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "        else:\n",
    "            traingroups_titledata[doc_group].append((doc_id, title))\n",
    "    return traingroups_titledata\n",
    "\n",
    "def preprocessing_3_old(traingroups_titledata, num_features=15, train=True):\n",
    "    y_train = []\n",
    "    X_train = []\n",
    "    groups_train = []\n",
    "    for new_group in traingroups_titledata:\n",
    "        docs = traingroups_titledata[new_group] \n",
    "        for k, tup in enumerate(docs):\n",
    "            if train:\n",
    "                doc_id, title, target_id = tup\n",
    "                y_train.append(target_id)\n",
    "            else:\n",
    "                doc_id, title = tup\n",
    "            groups_train.append(new_group)\n",
    "            all_dist = []\n",
    "            words = set(title.strip().split())\n",
    "            for j in range(0, len(docs)):\n",
    "                if k == j:\n",
    "                    continue\n",
    "                if train:\n",
    "                    doc_id_j, title_j, target_j = docs[j]\n",
    "                else:\n",
    "                    doc_id_j, title_j = docs[j]\n",
    "                words_j = set(title_j.strip().split())\n",
    "                all_dist.append(len(words.intersection(words_j)))\n",
    "            X_train.append(sorted(all_dist, reverse=True)[0:num_features])\n",
    "    if train:\n",
    "        return np.array(X_train), np.array(y_train), np.array(groups_train)\n",
    "    else:\n",
    "        return np.array(X_train), np.array([]), np.array(groups_train)\n",
    "    \n",
    "def preprocessing_3(traingroups_titledata, num_features=15, num_tfidf_features=30, max_df_tfidf=1.0, \\\n",
    "                    train=True):\n",
    "    y = []\n",
    "    X = []\n",
    "    groups = []\n",
    "    for new_group in traingroups_titledata:\n",
    "        docs = traingroups_titledata[new_group] \n",
    "        list_data = []\n",
    "        for k, tup in enumerate(docs):\n",
    "            if train:\n",
    "                doc_id, title, target_id = tup\n",
    "                y.append(target_id)\n",
    "            else:\n",
    "                doc_id, title = tup\n",
    "            list_data.append(title)  \n",
    "            groups.append(new_group)\n",
    "\n",
    "        vectorizer = TfidfVectorizer(max_features=num_tfidf_features, max_df=max_df_tfidf)\n",
    "        group_voc = vectorizer.fit_transform(list_data)\n",
    "        dist = cosine_similarity(group_voc, group_voc)\n",
    "        X_curr  = np.sort(dist, axis=1)[:,-(num_features+1):-1][:,::-1]\n",
    "        if X_curr.shape[1] < num_features:\n",
    "            residual = np.zeros((X_curr.shape[0], num_features - X_curr.shape[1]))\n",
    "            X_curr = np.hstack((X_curr, residual)) \n",
    "        X.append(X_curr)\n",
    "    X = np.vstack(X)\n",
    "    if train:\n",
    "        return np.array(X), np.array(y), np.array(groups)\n",
    "    else:\n",
    "        return np.array(X), np.array(groups)\n",
    "    \n",
    "def preprocessing(useful_words_tsv, train_or_test_groups_csv, min_length, num_features, num_tfidf_features, \n",
    "                  max_df_tfidf=1.0, train=True):\n",
    "    doc_to_title = preprocessing_1(useful_words_tsv, min_length=min_length)\n",
    "    traingroups_titledata = preprocessing_2(train_or_test_groups_csv, doc_to_title, train=train)\n",
    "    tup = preprocessing_3(traingroups_titledata, num_features=num_features, \\\n",
    "                          num_tfidf_features=num_tfidf_features, max_df_tfidf=max_df_tfidf, train=train)\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cross_validation_old(alpha, C, max_epoch, th, \\\n",
    "                           model_list, X_list, y, \\\n",
    "                           groups_train, kfold_generator, folds=10, verbose=False):    \n",
    "    total_score = 0.\n",
    "    total_ac_score = 0.\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        first_train_index, first_test_index, second_train_index, second_test_index = \\\n",
    "            train_index[0::2], test_index[0::2], \\\n",
    "            train_index[1::2], test_index[1::2]\n",
    "        \n",
    "        y_train, y_test = y[first_train_index], y[first_test_index]\n",
    "        y_pred_model = []\n",
    "        for model, X in zip(model_list, X_list):\n",
    "            X_train, X_test = X[first_train_index], X[first_test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_model.append(model.predict_proba(X_test)[:,1])\n",
    "        \n",
    "        X_train = np.vstack(y_pred_model).T       \n",
    "        total_model = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                          alpha=alpha, C=C, max_epoch=max_epoch, th=th)\n",
    "        total_model.fit(X_train, y_test)\n",
    "        \n",
    "        y_train, y_test = y[second_train_index], y[second_test_index]\n",
    "        y_pred_model = []\n",
    "        for model, X in zip(model_list, X_list):\n",
    "            X_train, X_test = X[second_train_index], X[second_test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_model.append(model.predict_proba(X_test)[:,1])\n",
    "\n",
    "        X_test = np.vstack(y_pred_model).T       \n",
    "        y_pred = total_model.predict(X_test)[:,1]\n",
    "        \n",
    "        score = f1_score(y_test, y_pred)\n",
    "        ac_score = accuracy_score(y_test, y_pred)\n",
    "        total_score += score\n",
    "        total_ac_score += ac_score\n",
    "        if verbose:\n",
    "            print(i, \"score:\", score)\n",
    "    mean_score = total_score / folds\n",
    "    mean_ac_score = total_ac_score / folds\n",
    "    if verbose:\n",
    "        print(\"MEAN_SCORE:\", mean_score)\n",
    "    return mean_score, mean_ac_score\n",
    "\n",
    "def total_grid_cv(alpha_list, C_list, max_epoch_list, th_list, \\\n",
    "                  model_list, X_list, y, \\\n",
    "                  groups_train, kfold_generator, batch_generator, \\\n",
    "                  folds=10, repeats=1, verbose=True):\n",
    "    sample_scores = np.array([])\n",
    "    sample_ac_scores = np.array([])\n",
    "    sample_params = []\n",
    "    for alpha in alpha_list:\n",
    "        for C in C_list:\n",
    "            for max_epoch in max_epoch_list:\n",
    "                for th in th_list: \n",
    "                    curr_mean_score_list = np.array([])\n",
    "                    curr_mean_ac_score_list = np.array([])\n",
    "                    for r in range(repeats):\n",
    "                        curr_score, curr_ac_score = total_cross_validation(alpha, C, max_epoch, th, \\\n",
    "                                                      model_list, X_list, y, \\\n",
    "                                                      groups_train, kfold_generator, folds=folds)\n",
    "                        curr_mean_score_list = np.append(curr_mean_score_list, curr_score)\n",
    "                        curr_mean_ac_score_list = np.append(curr_mean_ac_score_list, curr_ac_score)\n",
    "                    curr_mean_score = curr_mean_score_list.mean()\n",
    "                    curr_mean_ac_score = curr_mean_ac_score_list.mean()\n",
    "                    sample_scores = np.append(sample_scores, curr_mean_score)\n",
    "                    sample_ac_scores = np.append(sample_ac_scores, curr_mean_ac_score)\n",
    "                    sample_tuple = (alpha, C, max_epoch, th)\n",
    "                    sample_params.append(sample_tuple)\n",
    "                    if verbose:\n",
    "                        print(\"SCORE: %.5f\" % curr_mean_score, end='\\t')\n",
    "                        print(\"ACC: %.3f\" % curr_mean_ac_score, end='\\t')\n",
    "                        print(\"(alpha = %s; C = %s; max_epoch = %s; th = %s)\" % (alpha, C, max_epoch, th))\n",
    "    best_score_index = np.argmax(sample_scores)\n",
    "    best_score = sample_scores[best_score_index]\n",
    "    best_ac_score = sample_ac_scores[best_score_index]\n",
    "    best_params = sample_params[best_score_index]\n",
    "    if verbose:\n",
    "        print(\"\\nBEST SCORE:\\t\", best_score)\n",
    "        print(\"BEST PARAMS:\\t\", best_params)\n",
    "    return best_score, best_params, sample_scores, sample_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class total_model_class(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model_list, alpha=0.1, C=100, max_epoch=10, th=0.5):\n",
    "        self.model_list = model_list\n",
    "        self.th = th\n",
    "        self.weights = []\n",
    "        self.total_model = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                                                         alpha=alpha, C=C, max_epoch=max_epoch)\n",
    "    def fit(self, X_first, y_first):\n",
    "        X_train_list, X_test_list = X_first\n",
    "        y_train, y_test = y_first\n",
    "        y_pred_model = []\n",
    "        for model, X_train, X_test in zip(self.model_list, X_train_list, X_test_list):\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_model.append(model.predict_proba(X_test)[:,1])   \n",
    "        X_train_total = np.vstack(y_pred_model).T \n",
    "    \n",
    "        self.total_model.fit(X_train_total, y_test)\n",
    "        self.weights = self.total_model.weights\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X_second, y_second):\n",
    "        X_train_list, X_test_list = X_second\n",
    "        y_train = y_second\n",
    "        y_pred_model = []\n",
    "        for model, X_train, X_test in zip(self.model_list, X_train_list, X_test_list):\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_model.append(model.predict_proba(X_test)[:,1])\n",
    "        X_test_total = np.vstack(y_pred_model).T  \n",
    "\n",
    "        y_pred = self.total_model.predict_proba(X_test_total)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X_second, y_second):\n",
    "        y_pred = self.predict_proba(X_second, y_second)[:,1]\n",
    "        return (y_pred - self.th > 0).astype(int)\n",
    "\n",
    "def total_cross_validation(alpha, C, max_epoch, th, \\\n",
    "                           model_list, X_list, y, \\\n",
    "                           groups_train, kfold_generator, folds=10, verbose=False):    \n",
    "    total_score = 0.\n",
    "    total_ac_score = 0.\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        first_train_index, first_test_index, second_train_index, second_test_index = \\\n",
    "            train_index[0::2], test_index[0::2], \\\n",
    "            train_index[1::2], test_index[1::2]\n",
    "        \n",
    "        X_first_tr_list = []\n",
    "        X_first_tst_list = []\n",
    "        for X in X_list:\n",
    "            X_first_tr_list.append(X[first_train_index])\n",
    "            X_first_tst_list.append(X[first_test_index])\n",
    "        y_first = y[first_train_index], y[first_test_index]\n",
    "        X_first = X_first_tr_list, X_first_tst_list\n",
    "            \n",
    "        X_second_tr_list = []\n",
    "        X_second_tst_list = []\n",
    "        for X in X_list:\n",
    "            X_second_tr_list.append(X[second_train_index])\n",
    "            X_second_tst_list.append(X[second_test_index])\n",
    "        y_second = y[second_train_index]\n",
    "        X_second = X_second_tr_list, X_second_tst_list\n",
    "\n",
    "\n",
    "        total_model = total_model_class(model_list, alpha=alpha, C=C, max_epoch=max_epoch, th=th)\n",
    "        total_model.fit(X_first, y_first)\n",
    "        y_pred = total_model.predict(X_second, y_second)\n",
    "        \n",
    "        score = f1_score(y[second_test_index], y_pred)\n",
    "        ac_score = accuracy_score(y[second_test_index], y_pred)\n",
    "        total_score += score\n",
    "        total_ac_score += ac_score\n",
    "        if verbose:\n",
    "            print(i, \"score:\", score)\n",
    "    mean_score = total_score / folds\n",
    "    mean_ac_score = total_ac_score / folds\n",
    "    if verbose:\n",
    "        print(\"MEAN_SCORE:\", mean_score)\n",
    "    return mean_score, mean_ac_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fit мелких моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) title_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_tr_title, y_tr_title, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_title)\n",
    "X_tr_title_sc = scaler.transform(X_tr_title)\n",
    "print(X_tr_title_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.05, 0.1, 0.15]\n",
    "C_list = [100, 250, 500, 750, 1000]\n",
    "max_epoch_list = [8, 16]\n",
    "th_list = [0.27]\n",
    "max_features_list = [30, 25, 20, 15]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "                                    X_tr_title_sc, y_tr_title, groups_train, kfold_generator, batch_generator, \\\n",
    "                                    model_type='logreg', folds=6, repeats=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.68519\tACC: 0.808\t(alpha = 0.05; C = 100; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.67798\tACC: 0.807\t(alpha = 0.05; C = 100; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.67787\tACC: 0.797\t(alpha = 0.05; C = 100; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68146\tACC: 0.800\t(alpha = 0.05; C = 100; max_epoch = 8; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68933\tACC: 0.807\t(alpha = 0.05; C = 100; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68599\tACC: 0.807\t(alpha = 0.05; C = 100; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68648\tACC: 0.805\t(alpha = 0.05; C = 100; max_epoch = 16; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68436\tACC: 0.802\t(alpha = 0.05; C = 100; max_epoch = 16; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68400\tACC: 0.808\t(alpha = 0.05; C = 250; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68592\tACC: 0.806\t(alpha = 0.05; C = 250; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68604\tACC: 0.803\t(alpha = 0.05; C = 250; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68165\tACC: 0.802\t(alpha = 0.05; C = 250; max_epoch = 8; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68464\tACC: 0.808\t(alpha = 0.05; C = 250; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68546\tACC: 0.806\t(alpha = 0.05; C = 250; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68841\tACC: 0.805\t(alpha = 0.05; C = 250; max_epoch = 16; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68556\tACC: 0.802\t(alpha = 0.05; C = 250; max_epoch = 16; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68855\tACC: 0.808\t(alpha = 0.05; C = 500; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68805\tACC: 0.807\t(alpha = 0.05; C = 500; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.67652\tACC: 0.804\t(alpha = 0.05; C = 500; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68250\tACC: 0.801\t(alpha = 0.05; C = 500; max_epoch = 8; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68636\tACC: 0.808\t(alpha = 0.05; C = 500; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.69273\tACC: 0.807\t(alpha = 0.05; C = 500; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68129\tACC: 0.805\t(alpha = 0.05; C = 500; max_epoch = 16; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68695\tACC: 0.802\t(alpha = 0.05; C = 500; max_epoch = 16; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68512\tACC: 0.809\t(alpha = 0.05; C = 750; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68800\tACC: 0.806\t(alpha = 0.05; C = 750; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68451\tACC: 0.804\t(alpha = 0.05; C = 750; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68276\tACC: 0.802\t(alpha = 0.05; C = 750; max_epoch = 8; th = 0.27; max_features = 15)\n",
      "SCORE: 0.69053\tACC: 0.809\t(alpha = 0.05; C = 750; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68854\tACC: 0.807\t(alpha = 0.05; C = 750; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68579\tACC: 0.805\t(alpha = 0.05; C = 750; max_epoch = 16; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68678\tACC: 0.802\t(alpha = 0.05; C = 750; max_epoch = 16; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68533\tACC: 0.807\t(alpha = 0.05; C = 1000; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68797\tACC: 0.806\t(alpha = 0.05; C = 1000; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68606\tACC: 0.803\t(alpha = 0.05; C = 1000; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.67898\tACC: 0.801\t(alpha = 0.05; C = 1000; max_epoch = 8; th = 0.27; max_features = 15)\n",
      "SCORE: 0.69106\tACC: 0.809\t(alpha = 0.05; C = 1000; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68067\tACC: 0.806\t(alpha = 0.05; C = 1000; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68902\tACC: 0.805\t(alpha = 0.05; C = 1000; max_epoch = 16; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68910\tACC: 0.803\t(alpha = 0.05; C = 1000; max_epoch = 16; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68920\tACC: 0.807\t(alpha = 0.1; C = 100; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68458\tACC: 0.806\t(alpha = 0.1; C = 100; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68505\tACC: 0.807\t(alpha = 0.1; C = 100; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68293\tACC: 0.803\t(alpha = 0.1; C = 100; max_epoch = 8; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68608\tACC: 0.809\t(alpha = 0.1; C = 100; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68767\tACC: 0.805\t(alpha = 0.1; C = 100; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68438\tACC: 0.805\t(alpha = 0.1; C = 100; max_epoch = 16; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68333\tACC: 0.801\t(alpha = 0.1; C = 100; max_epoch = 16; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68443\tACC: 0.807\t(alpha = 0.1; C = 250; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68909\tACC: 0.807\t(alpha = 0.1; C = 250; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68816\tACC: 0.805\t(alpha = 0.1; C = 250; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68791\tACC: 0.803\t(alpha = 0.1; C = 250; max_epoch = 8; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68812\tACC: 0.806\t(alpha = 0.1; C = 250; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68069\tACC: 0.804\t(alpha = 0.1; C = 250; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68505\tACC: 0.805\t(alpha = 0.1; C = 250; max_epoch = 16; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68438\tACC: 0.802\t(alpha = 0.1; C = 250; max_epoch = 16; th = 0.27; max_features = 15)\n",
      "SCORE: 0.68910\tACC: 0.808\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68912\tACC: 0.807\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68699\tACC: 0.804\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 20)\n",
      "SCORE: 0.68702\tACC: 0.803\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 15)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-ef1d591d4add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n\u001b[1;32m      8\u001b[0m                                     \u001b[0mX_tr_title_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkfold_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                     model_type='logreg', folds=6, repeats=8, verbose=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-ef6a8d2d1ebd>\u001b[0m in \u001b[0;36mgrid_cv\u001b[0;34m(alpha_list, C_list, max_epoch_list, th_list, max_features_list, X, y, groups_train, kfold_generator, batch_generator, model_type, folds, repeats, verbose)\u001b[0m\n\u001b[1;32m     85\u001b[0m                             curr_score, curr_ac_score = cross_validation(model, groups_train, \\\n\u001b[1;32m     86\u001b[0m                                                                          \u001b[0mkfold_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                                                          folds=folds, th=th)\n\u001b[0m\u001b[1;32m     88\u001b[0m                             \u001b[0mcurr_mean_score_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_mean_score_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                             \u001b[0mcurr_mean_ac_score_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_mean_ac_score_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_ac_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ef6a8d2d1ebd>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(model, groups_train, kfold_generator, X, y, folds, th, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-16b9fa03625a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mbatch_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-16b9fa03625a>\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(self, X_batch, y_batch)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'logreg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m301\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                       \u001b[0;31m# наугад\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha_list = [0.05]\n",
    "C_list = [1000]\n",
    "max_epoch_list = [16]\n",
    "th_list = [0.27]\n",
    "max_features_list = [30]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "                                    X_tr_title_sc, y_tr_title, groups_train, kfold_generator, batch_generator, \\\n",
    "                                    model_type='logreg', folds=6, repeats=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "\n",
    "X_tr_title, y_tr_title, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features) \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_title)\n",
    "X_tr_title_sc = scaler.transform(X_tr_title)\n",
    "print(X_tr_title_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySGDClassifier(C=1000, alpha=0.05,\n",
       "                batch_generator=<function batch_generator at 0x7fcba341b158>,\n",
       "                batch_size=50, max_epoch=16, model_type='logreg', th=0.5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "C = 1000\n",
    "max_epoch = 16\n",
    "\n",
    "title_model = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                            alpha=alpha, C=C, max_epoch=max_epoch) \n",
    "title_model.fit(X_tr_title_sc, y_tr_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_pred = title_model.predict_proba(X_tr_title_sc)[:,1]\n",
    "y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) h1_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_tr_h1, y_tr_h1, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features) \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_h1)\n",
    "X_tr_h1_sc = scaler.transform(X_tr_h1)\n",
    "print(X_tr_h1_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.68128\tACC: 0.812\t(alpha = 0.05; C = 500; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.67205\tACC: 0.808\t(alpha = 0.05; C = 500; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.67564\tACC: 0.808\t(alpha = 0.05; C = 500; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.67766\tACC: 0.808\t(alpha = 0.05; C = 500; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68780\tACC: 0.810\t(alpha = 0.05; C = 1000; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.66877\tACC: 0.806\t(alpha = 0.05; C = 1000; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.64580\tACC: 0.784\t(alpha = 0.05; C = 1000; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68329\tACC: 0.808\t(alpha = 0.05; C = 1000; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68482\tACC: 0.810\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.67682\tACC: 0.810\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.67853\tACC: 0.809\t(alpha = 0.1; C = 500; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68653\tACC: 0.805\t(alpha = 0.1; C = 500; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68397\tACC: 0.808\t(alpha = 0.1; C = 1000; max_epoch = 8; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68478\tACC: 0.809\t(alpha = 0.1; C = 1000; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.68550\tACC: 0.808\t(alpha = 0.1; C = 1000; max_epoch = 16; th = 0.27; max_features = 30)\n",
      "SCORE: 0.68599\tACC: 0.808\t(alpha = 0.1; C = 1000; max_epoch = 16; th = 0.27; max_features = 25)\n",
      "\n",
      "BEST SCORE:\t 0.6878008214721809\n",
      "BEST PARAMS:\t (0.05, 1000, 0.27, 8, 30)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.1]\n",
    "C_list = [500]\n",
    "max_epoch_list = [8]\n",
    "th_list = [0.27]\n",
    "max_features_list = [25]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "                                    X_tr_h1_sc, y_tr_h1, groups_train, kfold_generator, batch_generator, \\\n",
    "                                    model_type='logreg', folds=6, repeats=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_tr_h1, y_tr_h1, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features = num_tfidf_features) \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_h1)\n",
    "X_tr_h1_sc = scaler.transform(X_tr_h1)\n",
    "print(X_tr_h1_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySGDClassifier(C=500, alpha=0.1,\n",
       "                batch_generator=<function batch_generator at 0x7fcba341b158>,\n",
       "                batch_size=50, max_epoch=8, model_type='logreg', th=0.5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "C = 500\n",
    "max_epoch = 8\n",
    "\n",
    "h1_model = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                            alpha=alpha, C=C, max_epoch=max_epoch) \n",
    "h1_model.fit(X_tr_h1_sc, y_tr_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = h1_model.predict_proba(X_tr_h1_sc)[:,1]\n",
    "y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## с) h1_model_pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words_tsv = 'upload/h1_pymorphy2.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_feature = 1000000\n",
    "\n",
    "X_tr_h1_pymorphy2, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 26)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2  # $ pip install pymorphy2\n",
    "  \n",
    "def pos(word, morth=pymorphy2.MorphAnalyzer()):\n",
    "    \"Return a likely part of speech for the *word*.\"\"\"\n",
    "    return morth.parse(word)[0].tag.POS\n",
    "  \n",
    "words = \"Однако я так и не смог закончить\".split()\n",
    "functors_pos = {'INTJ', 'PRCL', 'CONJ', 'PREP'} \n",
    "\n",
    "import re\n",
    "doc_to_title = {}\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "with open('docs_titles.csv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        r = [ i.lower() for i in re.split(r'\\W+',title) if i]\n",
    "        p = [re.sub(r'[^А-я]', '', i) for i in r]\n",
    "        p1 = list(filter(lambda i: i, p))\n",
    "        cur = [word for word in p1 if (pos(word) not in functors_pos and len(word)>1)]\n",
    "        cur = [morph.parse(o)[0].normal_form for o in cur]\n",
    "        title = ''\n",
    "        for i in cur:\n",
    "            if len(i)>2:\n",
    "                title += i + ' '        \n",
    "        doc_to_title[doc_id] = title\n",
    "        \n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "# y_train = []\n",
    "X_tr_h1_pymorphy2_add = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "#         y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        sum1 = 0\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            y = len(words.intersection(words_j))\n",
    "            all_dist.append(y)\n",
    "            sum1+=y\n",
    "        X_tr_h1_pymorphy2_add.append([sum1])\n",
    "X_tr_h1_pymorphy2_add = np.array(X_tr_h1_pymorphy2_add)\n",
    "# y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "\n",
    "X_tr_h1_pymorphy2 = np.hstack((X_tr_h1_pymorphy2, X_tr_h1_pymorphy2_add))\n",
    "X_tr_h1_pymorphy2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 26)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_h1_pymorphy2)\n",
    "X_tr_h1_sc_pymorphy2 = scaler.transform(X_tr_h1_pymorphy2)\n",
    "print(X_tr_h1_sc_pymorphy2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.71018\tACC: 0.823\t(alpha = 0.05; C = 100; max_epoch = 8; th = 0.28; max_features = 26)\n",
      "SCORE: 0.70494\tACC: 0.823\t(alpha = 0.05; C = 100; max_epoch = 16; th = 0.28; max_features = 26)\n",
      "SCORE: 0.70918\tACC: 0.825\t(alpha = 0.05; C = 500; max_epoch = 8; th = 0.28; max_features = 26)\n",
      "SCORE: 0.70519\tACC: 0.824\t(alpha = 0.05; C = 500; max_epoch = 16; th = 0.28; max_features = 26)\n",
      "\n",
      "BEST SCORE:\t 0.7101846701239516\n",
      "BEST PARAMS:\t (0.05, 100, 0.28, 8, 26)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.05]\n",
    "C_list = [100]\n",
    "max_epoch_list = [8]\n",
    "th_list = [0.28]\n",
    "max_features_list = [26]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "                                    X_tr_h1_sc_pymorphy2, y_tr_title, groups_train, kfold_generator, \\\n",
    "                                    batch_generator, model_type='logreg', folds=9, repeats=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.70383\tACC: 0.822\t(alpha = 0.05; C = 100; max_epoch = 8; th = 0.28; max_features = 26)\n",
      "SCORE: 0.70100\tACC: 0.824\t(alpha = 0.05; C = 100; max_epoch = 20; th = 0.28; max_features = 26)\n",
      "\n",
      "BEST SCORE:\t 0.7038262252396397\n",
      "BEST PARAMS:\t (0.05, 100, 0.28, 8, 26)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.05]\n",
    "C_list = [100]\n",
    "max_epoch_list = [8]\n",
    "th_list = [0.26, 0.28]\n",
    "max_features_list = [26]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "                                    X_tr_h1_sc_pymorphy2, y_tr_title, groups_train, kfold_generator, \\\n",
    "                                    batch_generator, model_type='logreg', folds=6, repeats=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySGDClassifier(C=100, alpha=0.05,\n",
       "                batch_generator=<function batch_generator at 0x7fcba341b158>,\n",
       "                batch_size=50, max_epoch=8, model_type='logreg', th=0.5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "C = 100\n",
    "max_epoch = 8\n",
    "\n",
    "h1_model_pymorphy2 = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                            alpha=alpha, C=C, max_epoch=max_epoch) \n",
    "h1_model_pymorphy2.fit(X_tr_h1_sc_pymorphy2, y_tr_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = h1_model_pymorphy2.predict_proba(X_tr_h1_sc_pymorphy2)[:,1]\n",
    "y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) RForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 15\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_train_tree, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features) \n",
    "print(X_train_tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 30)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 15\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_train_addition_tree, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                       min_length=min_length, num_features=num_features, \\\n",
    "                                       num_tfidf_features=num_tfidf_features) \n",
    "X_train_tree = np.hstack((X_train_tree, X_train_addition_tree))\n",
    "X_train_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/useful_names.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 0\n",
    "num_features = 2\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "\n",
    "X_train_addition_tree, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                       min_length=min_length, num_features=num_features, \\\n",
    "                                       num_tfidf_features = num_tfidf_features) \n",
    "X_train_tree = np.hstack((X_train_tree, X_train_addition_tree))\n",
    "X_train_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 34)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h2_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 2\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                       min_length=min_length, num_features=num_features, \\\n",
    "                                       num_tfidf_features=num_tfidf_features) \n",
    " \n",
    "X_train_tree = np.hstack((X_train_tree, X_train_addition_tree))\n",
    "X_train_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(name, input_file, tr_or_tst_groups):\n",
    "    feature = pd.read_csv(input_file, sep='\\t', encoding='utf-8', lineterminator='\\n')\n",
    "    group_df = pd.read_csv(tr_or_tst_groups, sep=',', encoding='utf-8', lineterminator='\\n')\n",
    "    feature = pd.merge(feature, group_df, on='doc_id', how='right', sort=True).dropna()\n",
    "    feature['mean_'+name] = feature.groupby('group_id')[name].transform('mean')\n",
    "    feature = feature[['mean_'+name, 'pair_id',name]]\n",
    "    feature['pair_id'] = feature['pair_id'].astype(int)    \n",
    "    feature = pd.merge(group_df, feature, on='pair_id', how='left', sort=True)\n",
    "    feature['diff_'+name] = np.abs(feature['mean_'+name] - feature[name])\n",
    "\n",
    "    return np.asmatrix(feature['diff_'+name].to_numpy()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 35)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'num_images'\n",
    "input_file = 'upload/pics_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "X_train_addition_tree = do_features(name, input_file, tr_or_tst_groups)\n",
    "\n",
    "X_train_tree = np.hstack((X_train_tree, X_train_addition_tree))\n",
    "X_train_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 36)\n",
      "(11690, 37)\n",
      "(11690, 38)\n",
      "(11690, 39)\n",
      "(11690, 40)\n",
      "(11690, 41)\n",
      "(11690, 42)\n"
     ]
    }
   ],
   "source": [
    "input_file = 'upload/digits_output_statistics.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "\n",
    "for name in ['number_cntr', 'year_psbl', 'near_year_psbl', 'useful_num_ratio', \\\n",
    "          'long_numbers', 'long_num_ratio', 'short_numbers']:\n",
    "    X_train_addition_tree = do_features(name, input_file, tr_or_tst_groups)\n",
    "    X_train_tree = np.hstack((X_train_tree, X_train_addition_tree))\n",
    "    print(X_train_tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 43)\n",
      "(11690, 44)\n",
      "(11690, 45)\n",
      "(11690, 46)\n"
     ]
    }
   ],
   "source": [
    "input_file = 'upload/punctuation_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "\n",
    "for name in ['excl_mrk_num', 'ques_mrk_num', 'poin_mrk_num', 'ellp_mrk_num']:\n",
    "    X_train_addition_tree = do_features(name, input_file, tr_or_tst_groups)\n",
    "    X_train_tree = np.hstack((X_train_tree, X_train_addition_tree))\n",
    "    print(X_train_tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 47)\n"
     ]
    }
   ],
   "source": [
    "name = 'file_size'\n",
    "input_file = 'upload/size_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "X_train_addition_tree = do_features(name, input_file, tr_or_tst_groups)\n",
    "X_train_tree = np.hstack((X_train_tree, X_train_addition_tree))\n",
    "print(X_train_tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_tree)\n",
    "X_train_scale_tree = scaler.transform(X_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=10, max_features=14, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=4, min_samples_split=9,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tree = RandomForestClassifier(max_depth=10, min_samples_split=9, n_estimators=200,\\\n",
    "                   min_samples_leaf=4, max_features=14, criterion='gini')\n",
    "model_tree.fit(X_train_scale_tree, y_tr_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_tree.predict_proba(X_train_scale_tree)[:,1]\n",
    "y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) allwords_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n",
      "CPU times: user 3min 35s, sys: 3.54 s, total: 3min 38s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# useful_words_tsv = 'upload/allwords_mystep_output.txt'\n",
    "# train_or_test_groups_csv = 'train_groups.csv'\n",
    "# min_length = 3\n",
    "# num_features = 25\n",
    "# num_tfidf_features = 10000000\n",
    "# max_df_tfidf = 1.0\n",
    "\n",
    "# X_tr_aw, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "#                                               min_length=min_length, num_features=num_features, \\\n",
    "#                                               num_tfidf_features=num_tfidf_features, max_df_tfidf=max_df_tfidf) \n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_tr_aw)\n",
    "# X_tr_aw_sc = scaler.transform(X_tr_aw)\n",
    "# print(X_tr_aw_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.49634\tACC: 0.646\t(alpha = 0.05; C = 500; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "SCORE: 0.46776\tACC: 0.605\t(alpha = 0.15; C = 500; max_epoch = 8; th = 0.27; max_features = 25)\n",
      "\n",
      "BEST SCORE:\t 0.4963441531440289\n",
      "BEST PARAMS:\t (0.05, 500, 0.27, 8, 25)\n"
     ]
    }
   ],
   "source": [
    "# alpha_list = [0.05, 0.15]\n",
    "# C_list = [500]\n",
    "# max_epoch_list = [8]\n",
    "# th_list = [0.27]\n",
    "# max_features_list = [25]\n",
    "\n",
    "# best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "#                                     X_tr_aw_sc, y_tr_h1, groups_train, kfold_generator, batch_generator, \\\n",
    "#                                     model_type='logreg', folds=6, repeats=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "# train_or_test_groups_csv = 'train_groups.csv'\n",
    "# min_length = 3\n",
    "# num_features = 25\n",
    "# num_tfidf_features = 100000\n",
    "\n",
    "# X_tr_aw_sc, y_tr_h1, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "#                                               min_length=min_length, num_features=num_features, \\\n",
    "#                                               num_tfidf_features = num_tfidf_features) \n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_tr_h1)\n",
    "# X_tr_h1_sc = scaler.transform(X_tr_h1)\n",
    "# print(X_tr_h1_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.1\n",
    "# C = 500\n",
    "# max_epoch = 8\n",
    "\n",
    "# allword_model = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "#                             alpha=alpha, C=C, max_epoch=max_epoch) \n",
    "# allword_model.fit(X_tr_aw_sc, y_tr_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = allword_model.predict_proba(X_tr_aw_sc)[:,1]\n",
    "# y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) title_pym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "# useful_words_tsv = 'upload/title_output_pymorphy2.txt'\n",
    "# train_or_test_groups_csv = 'train_groups.csv'\n",
    "# min_length = 3\n",
    "# num_features = 25\n",
    "# num_tfidf_features = 10000000\n",
    "\n",
    "# X_tr_title_pym, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "#                                               min_length=min_length, num_features=num_features, \\\n",
    "#                                               num_tfidf_features=num_tfidf_features) \n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_tr_title_pym)\n",
    "# X_tr_title_sc_pym = scaler.transform(X_tr_title_pym)\n",
    "# print(X_tr_title_sc_pym.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_list = [0.05, 0.1, 0.15]\n",
    "# C_list = [100, 250, 500, 750, 1000]\n",
    "# max_epoch_list = [8, 16]\n",
    "# th_list = [0.27]\n",
    "# max_features_list = [30, 25, 20, 15]\n",
    "\n",
    "# best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "#                                     X_tr_title_sc_pym, y_tr_title, groups_train, kfold_generator, \\\n",
    "#                                     batch_generator, model_type='logreg', folds=6, repeats=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 25)\n"
     ]
    }
   ],
   "source": [
    "# useful_words_tsv = 'upload/title_output_pymorphy2.txt'\n",
    "# train_or_test_groups_csv = 'train_groups.csv'\n",
    "# min_length = 3\n",
    "# num_features = 25\n",
    "# num_tfidf_features = 1000000\n",
    "\n",
    "\n",
    "# X_tr_title_pym, y_tr_title, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "#                                               min_length=min_length, num_features=num_features, \\\n",
    "#                                               num_tfidf_features=num_tfidf_features) \n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_tr_title_pym)\n",
    "# X_tr_title_sc_pym = scaler.transform(X_tr_title_pym)\n",
    "# print(X_tr_title_sc_pym.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySGDClassifier(C=1000, alpha=0.07,\n",
       "                batch_generator=<function batch_generator at 0x7fcba341b158>,\n",
       "                batch_size=50, max_epoch=16, model_type='logreg', th=0.5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha = 0.07\n",
    "# C = 1000\n",
    "# max_epoch = 16\n",
    "\n",
    "# title_model_pym = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "#                             alpha=alpha, C=C, max_epoch=max_epoch) \n",
    "# title_model_pym.fit(X_tr_title_sc_pym, y_tr_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = title_model_pym.predict_proba(X_tr_title_sc_pym)[:,1]\n",
    "# y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 10)\n"
     ]
    }
   ],
   "source": [
    "# useful_words_tsv = 'upload/digits_output_numbers.txt'\n",
    "# train_or_test_groups_csv = 'train_groups.csv'\n",
    "# min_length = 3\n",
    "# num_features = 10\n",
    "# num_tfidf_features = 10000000\n",
    "\n",
    "# X_g, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "#                                               min_length=min_length, num_features=num_features, \\\n",
    "#                                               num_tfidf_features=num_tfidf_features) \n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_g)\n",
    "# X_g_sc = scaler.transform(X_g)\n",
    "# print(X_g_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_list = [0.05, 0.1, 0.15]\n",
    "# C_list = [100, 250, 500, 750, 1000]\n",
    "# max_epoch_list = [8, 16]\n",
    "# th_list = [0.27]\n",
    "# max_features_list = [30, 25, 20, 15]\n",
    "\n",
    "# best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, max_features_list, \\\n",
    "#                                     X_g, y_tr_title, groups_train, kfold_generator, batch_generator, \\\n",
    "#                                     model_type='logreg', folds=6, repeats=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful_words_tsv = 'upload/articles_output.txt'\n",
    "# train_or_test_groups_csv = 'train_groups.csv'\n",
    "# min_length = 3\n",
    "# num_features = 30\n",
    "# num_tfidf_features = 1000000\n",
    "\n",
    "\n",
    "# X_tr_title, y_tr_title, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "#                                               min_length=min_length, num_features=num_features, \\\n",
    "#                                               num_tfidf_features=num_tfidf_features) \n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_tr_title)\n",
    "# X_tr_title_sc = scaler.transform(X_tr_title)\n",
    "# print(X_tr_title_sc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fit общей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [title_model, h1_model, h1_model_pymorphy2, model_tree]\n",
    "X_list = [X_tr_h1_sc, X_tr_title_sc, X_tr_h1_sc_pymorphy2, X_train_scale_tree]\n",
    "y_train = y_tr_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.71934\tACC: 0.822\t(alpha = 0.05; C = 1000; max_epoch = 45; th = 0.28)\n",
      "\n",
      "BEST SCORE:\t 0.7193407111640672\n",
      "BEST PARAMS:\t (0.05, 1000, 45, 0.28)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.05]\n",
    "C_list = [1000]\n",
    "max_epoch_list = [45]\n",
    "th_list = [0.28]\n",
    "\n",
    "best_score, _, sample_scores, _ = total_grid_cv(alpha_list, C_list, max_epoch_list, th_list, \\\n",
    "                                    model_list, X_list, y_train, \\\n",
    "                                    groups_train, kfold_generator, batch_generator, \\\n",
    "                                    folds=3, repeats=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7081959229799023, 0.7115522901095734)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scores[:18].mean(), sample_scores[18:].mean(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.71825\tACC: 0.827\t(alpha = 0.15; C = 500; max_epoch = 45; th = 0.28)\n",
      "SCORE: 0.71597\tACC: 0.826\t(alpha = 0.15; C = 500; max_epoch = 60; th = 0.28)\n",
      "SCORE: 0.71093\tACC: 0.825\t(alpha = 0.15; C = 500; max_epoch = 75; th = 0.28)\n",
      "\n",
      "BEST SCORE:\t 0.7182516699848973\n",
      "BEST PARAMS:\t (0.15, 500, 45, 0.28)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.15]\n",
    "C_list = [500]\n",
    "max_epoch_list = [45, 60, 75]\n",
    "th_list = [0.28]\n",
    "\n",
    "best_score, _, sample_scores, _ = total_grid_cv(alpha_list, C_list, max_epoch_list, th_list, \\\n",
    "                                    model_list, X_list, y_train, \\\n",
    "                                    groups_train, kfold_generator, batch_generator, \\\n",
    "                                    folds=3, repeats=16, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.70591\tACC: 0.813\t(alpha = 0.15; C = 500; max_epoch = 10; th = 0.27)\n",
      "SCORE: 0.70710\tACC: 0.818\t(alpha = 0.15; C = 500; max_epoch = 10; th = 0.28)\n",
      "SCORE: 0.68521\tACC: 0.817\t(alpha = 0.15; C = 500; max_epoch = 10; th = 0.29)\n",
      "SCORE: 0.70672\tACC: 0.824\t(alpha = 0.15; C = 500; max_epoch = 30; th = 0.27)\n",
      "SCORE: 0.70680\tACC: 0.823\t(alpha = 0.15; C = 500; max_epoch = 30; th = 0.28)\n",
      "SCORE: 0.69715\tACC: 0.821\t(alpha = 0.15; C = 500; max_epoch = 30; th = 0.29)\n",
      "\n",
      "BEST SCORE:\t 0.7070969040158275\n",
      "BEST PARAMS:\t (0.15, 500, 10, 0.28)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.15]\n",
    "C_list = [500]\n",
    "max_epoch_list = [10, 30]\n",
    "th_list = [0.27, 0.28, 0.29]\n",
    "\n",
    "best_score, _, sample_scores, _ = total_grid_cv(alpha_list, C_list, max_epoch_list, th_list, \\\n",
    "                                    model_list, X_list, y_train, \\\n",
    "                                    groups_train, kfold_generator, batch_generator, \\\n",
    "                                    folds=3, repeats=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### нахождение весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "C = 1000\n",
    "max_epoch = 45\n",
    "th = 0.28\n",
    "\n",
    "total_model = total_model_class(model_list, alpha=alpha, C=C, max_epoch=max_epoch, th=th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9462329 ,  0.76229288,  1.28617545,  1.70001948,  2.27410306])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeats = 1\n",
    "folds = 3\n",
    "weights_list = []\n",
    "\n",
    "for rep in range(repeats):\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        X_first_tr_list = []\n",
    "        X_first_tst_list = []\n",
    "        for X in X_list:\n",
    "            X_first_tr_list.append(X[train_index])\n",
    "            X_first_tst_list.append(X[test_index])\n",
    "        y_first = y_train[train_index], y_train[test_index]\n",
    "        X_first = X_first_tr_list, X_first_tst_list\n",
    "\n",
    "        total_model = total_model_class(model_list, alpha=alpha, C=C, max_epoch=max_epoch, th=th)\n",
    "        total_model.fit(X_first, y_first)\n",
    "\n",
    "        weights_list.append(total_model.weights)\n",
    "weights = np.vstack(weights_list).mean(axis=0)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(weights, y_pred_list, th=0.5):\n",
    "    X = np.vstack(y_pred_list)\n",
    "    X = np.vstack((np.ones(X.shape[1]), X)).T\n",
    "    \n",
    "    dot_func = lambda x: sigmoid(np.dot(x, weights))\n",
    "    y_pred = np.apply_along_axis(dot_func, 1, X)\n",
    "    return (y_pred - th > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (11691,) and (6,) not aligned: 11691 (dim 0) != 6 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-c1f030c95019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-9b78235453d6>\u001b[0m in \u001b[0;36mdo_prediction\u001b[0;34m(weights, y_pred_list, th)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdot_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot apply_along_axis when any iteration dimensions are 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;31m# build a buffer for storing evaluations of func1d.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-9b78235453d6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdot_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (11691,) and (6,) not aligned: 11691 (dim 0) != 6 (dim 0)"
     ]
    }
   ],
   "source": [
    "pr = do_prediction(weights, y_pred_list[0], th=0.27).astype(int)\n",
    "f1_score(pr, y_train), \\\n",
    "accuracy_score(pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.97507427,  0.6932635 ,  0.82046305,  1.53893646,  2.20042164,\n",
       "        0.83150082])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.87688254,  0.31756383,  1.12051629,  1.79253145,  2.47718505])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.67100513,  1.57516094,  2.3974179 ,  1.55038593])"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.53331013,  2.28159524,  2.79158732])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.69508415,  2.54027195,  3.01626598])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.43584684,  0.6779059 ,  0.78940243])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, test_groups_csv, out_file, target='target', index_label=\"pair_id\"):\n",
    "    indices = np.asarray(pd.read_csv(test_groups_csv)[index_label])\n",
    "    predicted_df = pd.DataFrame(predicted_labels, index = indices, columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка тестовых фичей для total_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) X_tst_title_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 100000\n",
    "\n",
    "X_tst_title, groups_test = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features, train=False) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tst_title)\n",
    "X_tst_title_sc = scaler.transform(X_tst_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 25) 1\n"
     ]
    }
   ],
   "source": [
    "# repeats = 20\n",
    "# y_pred = []\n",
    "# for i in range(repeats):\n",
    "#     y_pred.append(title_model.predict_proba(X_tst_title_sc)[:,1])\n",
    "# y_pred = np.mean(y_pred)\n",
    "y_pred = title_model.predict_proba(X_tst_title_sc)[:,1]\n",
    "y_pred_list_test.append(y_pred)\n",
    "print(X_tst_title_sc.shape, len(y_pred_list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) X_tst_h1_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_tst_h1, groups_test = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features, train=False) \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tst_h1)\n",
    "X_tst_h1_sc = scaler.transform(X_tst_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 25) 2\n"
     ]
    }
   ],
   "source": [
    "# repeats = 20\n",
    "# y_pred = []\n",
    "# for i in range(repeats):\n",
    "#     y_pred.append(h1_model.predict_proba(X_tst_h1_sc)[:,1])\n",
    "# y_pred = np.mean(y_pred)\n",
    "y_pred = h1_model.predict_proba(X_tst_h1_sc)[:,1]\n",
    "y_pred_list_test.append(y_pred)\n",
    "print(X_tst_h1_sc.shape, len(y_pred_list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words_tsv = 'upload/h1_pymorphy2.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 25\n",
    "num_tfidf_feature = 1000000\n",
    "\n",
    "X_tst_h1_pymorphy2,  _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features, train=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 26)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2  # $ pip install pymorphy2\n",
    "  \n",
    "def pos(word, morth=pymorphy2.MorphAnalyzer()):\n",
    "    \"Return a likely part of speech for the *word*.\"\"\"\n",
    "    return morth.parse(word)[0].tag.POS\n",
    "  \n",
    "words = \"Однако я так и не смог закончить\".split()\n",
    "functors_pos = {'INTJ', 'PRCL', 'CONJ', 'PREP'} \n",
    "\n",
    "\n",
    "import re\n",
    "doc_to_title = {}\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "with open('docs_titles.csv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        r = [ i.lower() for i in re.split(r'\\W+',title) if i]\n",
    "        p = [re.sub(r'[^А-я]', '', i) for i in r]\n",
    "        p1 = list(filter(lambda i: i, p))\n",
    "        cur = [word for word in p1 if (pos(word) not in functors_pos and len(word)>1)]\n",
    "        cur = [morph.parse(o)[0].normal_form for o in cur]\n",
    "        title = ''\n",
    "        for i in cur:\n",
    "            if len(i)>2:\n",
    "                title += i + ' '\n",
    "#         print(title)\n",
    "        \n",
    "        doc_to_title[doc_id] = title\n",
    "# print (len(doc_to_title))\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "test_data = pd.read_csv('test_groups.csv')\n",
    "test_groups_titledata = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc1 = test_data.iloc[i]\n",
    "    doc_group1 = new_doc1['group_id']\n",
    "    doc_id1 = new_doc1['doc_id']\n",
    "    title1 = doc_to_title[doc_id1]\n",
    "    if doc_group1 not in test_groups_titledata:\n",
    "        test_groups_titledata[doc_group1] = []\n",
    "    test_groups_titledata[doc_group1].append((doc_id1, title1))\n",
    "X_tst_h1_pymorphy2_add = []\n",
    "groups_test = []\n",
    "for new_group in test_groups_titledata:\n",
    "    docs1 = test_groups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs1):\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        sum1 = 0\n",
    "        for j in range(0, len(docs1)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j= docs1[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            y = len(words.intersection(words_j))\n",
    "            all_dist.append(y)\n",
    "            sum1+=y\n",
    "#             all_dist.append(len(words.intersection(words_j)))\n",
    "        X_tst_h1_pymorphy2_add.append([sum1])\n",
    "X_tst_h1_pymorphy2_add = np.array(X_tst_h1_pymorphy2_add)\n",
    "groups_test = np.array(groups_test)\n",
    "\n",
    "X_tst_h1_pymorphy2 = np.hstack((X_tst_h1_pymorphy2, X_tst_h1_pymorphy2_add))\n",
    "X_tst_h1_pymorphy2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tst_h1_pymorphy2)\n",
    "X_tst_h1_pymorphy2_sc = scaler.transform(X_tst_h1_pymorphy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 26) 3\n"
     ]
    }
   ],
   "source": [
    "# repeats = 20\n",
    "# y_pred = []\n",
    "# for i in range(repeats):\n",
    "#     y_pred.append(h1_model_pymorphy2.predict_proba(X_tst_h1_pymorphy2_sc)[:,1])\n",
    "# y_pred = np.mean(y_pred)\n",
    "y_pred = h1_model_pymorphy2.predict_proba(X_tst_h1_pymorphy2_sc)[:,1]\n",
    "y_pred_list_test.append(y_pred)\n",
    "print(X_tst_h1_pymorphy2_sc.shape, len(y_pred_list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 15)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 15\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_test_tree, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features, train=False) \n",
    "print(X_test_tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 30)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 15\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_test_addition_tree, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                       min_length=min_length, num_features=num_features, \\\n",
    "                                       num_tfidf_features=num_tfidf_features, train=False) \n",
    "X_test_tree = np.hstack((X_test_tree, X_test_addition_tree))\n",
    "X_test_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/useful_names.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 0\n",
    "num_features = 2\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "\n",
    "X_test_addition_tree, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                       min_length=min_length, num_features=num_features, \\\n",
    "                                       num_tfidf_features = num_tfidf_features, train=False) \n",
    "X_test_tree = np.hstack((X_test_tree, X_test_addition_tree))\n",
    "X_test_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 34)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h2_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 2\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_test_addition_tree, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                       min_length=min_length, num_features=num_features, \\\n",
    "                                       num_tfidf_features=num_tfidf_features, train=False) \n",
    " \n",
    "X_test_tree = np.hstack((X_test_tree, X_test_addition_tree))\n",
    "X_test_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 35)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'num_images'\n",
    "input_file = 'upload/pics_output.txt'\n",
    "tr_or_tst_groups = 'test_groups.csv'\n",
    "X_test_addition_tree = do_features(name, input_file, tr_or_tst_groups)\n",
    "\n",
    "X_test_tree = np.hstack((X_test_tree, X_test_addition_tree))\n",
    "X_test_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 36)\n",
      "(16627, 37)\n",
      "(16627, 38)\n",
      "(16627, 39)\n",
      "(16627, 40)\n",
      "(16627, 41)\n",
      "(16627, 42)\n"
     ]
    }
   ],
   "source": [
    "input_file = 'upload/digits_output_statistics.txt'\n",
    "tr_or_tst_groups = 'test_groups.csv'\n",
    "\n",
    "for name in ['number_cntr', 'year_psbl', 'near_year_psbl', 'useful_num_ratio', \\\n",
    "          'long_numbers', 'long_num_ratio', 'short_numbers']:\n",
    "    X_test_addition_tree = do_features(name, input_file, tr_or_tst_groups)\n",
    "    X_test_tree = np.hstack((X_test_tree, X_test_addition_tree))\n",
    "    print(X_test_tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 43)\n",
      "(16627, 44)\n",
      "(16627, 45)\n",
      "(16627, 46)\n"
     ]
    }
   ],
   "source": [
    "input_file = 'upload/punctuation_output.txt'\n",
    "tr_or_tst_groups = 'test_groups.csv'\n",
    "\n",
    "for name in ['excl_mrk_num', 'ques_mrk_num', 'poin_mrk_num', 'ellp_mrk_num']:\n",
    "    X_test_addition_tree = do_features(name, input_file, tr_or_tst_groups)\n",
    "    X_test_tree = np.hstack((X_test_tree, X_test_addition_tree))\n",
    "    print(X_test_tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 47)\n"
     ]
    }
   ],
   "source": [
    "name = 'file_size'\n",
    "input_file = 'upload/size_output.txt'\n",
    "tr_or_tst_groups = 'test_groups.csv'\n",
    "X_test_addition_tree = do_features(name, input_file, tr_or_tst_groups)\n",
    "X_test_tree = np.hstack((X_test_tree, X_test_addition_tree))\n",
    "print(X_test_tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test_tree)\n",
    "X_test_scale_tree = scaler.transform(X_test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 47) 4\n"
     ]
    }
   ],
   "source": [
    "# repeats = 20\n",
    "# y_pred = []\n",
    "# for i in range(repeats):\n",
    "#     y_pred.append(model_tree.predict_proba(X_test_scale_tree)[:,1])\n",
    "# y_pred = np.mean(y_pred)\n",
    "y_pred = model_tree.predict_proba(X_test_scale_tree)[:,1]\n",
    "y_pred_list_test.append(y_pred)\n",
    "print(X_test_scale_tree.shape, len(y_pred_list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 27)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_pymorphy2.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 27\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_tst_title_pym, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features, train=False) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tst_title_pym)\n",
    "X_tst_title_sc_pym = scaler.transform(X_tst_title_pym)\n",
    "print(X_tst_title_sc_pym.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50529908, 0.20774718, 0.26600698, ..., 0.6791484 , 0.77497795,\n",
       "       0.54008344])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.vstack((title_model_pym.predict_proba(X_tst_title_sc_pym)[:,1],title_model_pym.predict_proba(X_tst_title_sc_pym)[:,1]))\n",
    "np.mean(t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 27) 5\n"
     ]
    }
   ],
   "source": [
    "repeats = 20\n",
    "# y_pred = []\n",
    "# for i in range(repeats):\n",
    "#     np.vstack((y_pred, title_model_pym.predict_proba(X_tst_title_sc_pym)[:,1]))\n",
    "# y_pred = np.mean(y_pred, axix=0)\n",
    "y_pred = title_model_pym.predict_proba(X_tst_title_sc_pym)[:,1]\n",
    "\n",
    "y_pred_list_test.append(y_pred)\n",
    "print(X_tst_title_sc_pym.shape, len(y_pred_list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(weights, y_pred_list, th=0.5):\n",
    "    X = np.vstack(y_pred_list)\n",
    "    X = np.vstack((np.ones(X.shape[1]), X)).T\n",
    "    \n",
    "    dot_func = lambda x: sigmoid(np.dot(x, weights))\n",
    "    y_pred = np.apply_along_axis(dot_func, 1, X)\n",
    "    return (y_pred - th > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10912,  5715])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(do_prediction(weights, y_pred_list_test, th=0.26).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-174-2f25baf48800>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-174-2f25baf48800>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sigmoid(np.dot(y_pred_list_test, weights)\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "sigmoid(np.dot(y_pred_list_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2956894386523128,\n",
       " 0.27941315979716563,\n",
       " 0.28597818387219953,\n",
       " 0.3000147979140002,\n",
       " 0.2867739597008345]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = do_prediction(weights, y_pred_list_test, th=0.26).astype(int)\n",
    "write_to_submission_file(y_pred, 'test_groups.csv', \"y_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "nastya = pd.read_csv('Anastasia/predict_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "nastya['my_pred'] = y_pred.astype(int)\n",
    "nastya['match'] = (nastya.target == nastya.my_pred).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1818, 14809])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(nastya.match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11057,  5570])"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11945,  4682])"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(nastya.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8329, 3361])"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
