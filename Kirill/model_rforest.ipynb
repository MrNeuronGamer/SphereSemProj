{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение каких-то функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_generator(groups_train, n_splits=10):\n",
    "    all_groups = np.unique(groups_train)\n",
    "    fold_size = len(all_groups) // n_splits\n",
    "    all_groups = np.random.permutation(all_groups)\n",
    "    fold_groups = np.zeros((n_splits,fold_size), dtype=int)\n",
    "    for i, group in enumerate(all_groups):\n",
    "        fold = i // fold_size\n",
    "        if fold == n_splits:\n",
    "            break\n",
    "        group_i = i % fold_size\n",
    "        fold_groups[fold,group_i] = group\n",
    "    fold_indices = {}\n",
    "    for fold in range(n_splits):\n",
    "        indices = np.array([], dtype = int)\n",
    "        for group in fold_groups[fold]:\n",
    "            indices = np.append(indices, np.argwhere(groups_train == group))\n",
    "        fold_indices[fold] = indices\n",
    "\n",
    "    for i in fold_indices:\n",
    "        kf_test = fold_indices[i]\n",
    "        kf_train = np.array([],dtype=int)\n",
    "        for j in fold_indices:\n",
    "            if i == j:\n",
    "                continue\n",
    "            kf_train = np.append(kf_train, fold_indices[j])\n",
    "        kf_tuple = [kf_train, kf_test]\n",
    "        yield (kf_train, kf_test)\n",
    "        \n",
    "def cross_validation(model, groups_train, kfold_generator, X, y, \\\n",
    "                     folds=10, th=0.5, verbose=False):    \n",
    "    total_score = 0\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_test)[:,1]  \n",
    "            \n",
    "        score = f1_score(y_test, (y_pred - th > 0))\n",
    "        total_score += score\n",
    "        if verbose:\n",
    "            print(i, \"score:\", score)\n",
    "    mean_score = total_score / folds\n",
    "    if verbose:\n",
    "        print(\"MEAN_SCORE:\", mean_score)\n",
    "    return mean_score\n",
    "\n",
    "def grid_cv(criterion_list, min_impurity_decrease_list, max_features_list, \\\n",
    "            min_samples_leaf_list, n_estimators_list, \\\n",
    "            min_samples_split_list, max_depth_list, th_list, \\\n",
    "            X, y, groups_train, kfold_generator, \\\n",
    "            folds=10, repeats=1, verbose=True):\n",
    "    sample_scores = np.array([])\n",
    "    sample_params = []\n",
    "    for criterion in criterion_list:\n",
    "        for min_impurity_decrease in min_impurity_decrease_list:\n",
    "            for max_features in max_features_list:\n",
    "                for min_samples_leaf in min_samples_leaf_list:\n",
    "                    for n_estimators in n_estimators_list:\n",
    "                        for min_samples_split in min_samples_split_list:\n",
    "                            for max_depth in max_depth_list:\n",
    "                                for th in th_list:\n",
    "                                    curr_mean_score_list = np.array([])\n",
    "                                    for r in range(repeats):\n",
    "                                        model = RandomForestClassifier(random_state=0, \\\n",
    "                                                                     max_depth=max_depth, \\\n",
    "                                                                     criterion=criterion, \\\n",
    "                                                                     min_samples_split=min_samples_split, \\\n",
    "                                                                     n_estimators=n_estimators, \\\n",
    "                                                                     min_samples_leaf=min_samples_leaf, \\\n",
    "                                                                     max_features=max_features)\n",
    "                                        curr_score = cross_validation(model, groups_train, kfold_generator, \\\n",
    "                                                                      X, y, folds=folds, th=th)\n",
    "                                        curr_mean_score_list = np.append(curr_mean_score_list, curr_score)\n",
    "                                    curr_mean_score = curr_mean_score_list.mean()\n",
    "                                    sample_scores = np.append(sample_scores, curr_mean_score)\n",
    "                                    sample_tuple = (criterion, min_impurity_decrease, max_features, \\\n",
    "                                                    min_samples_leaf, n_estimators, \\\n",
    "                                                    min_samples_split, max_depth, th)\n",
    "                                    sample_params.append(sample_tuple)\n",
    "                                    if verbose:\n",
    "                                        print(\"SCORE: \", curr_mean_score, end='\\t')\n",
    "#                                         print(\"(%s; minID = %s; maxF = %s; minSL = %s; nE = %s; minSS= %s; maxD = %s; th = %s)\" \\\n",
    "                                        print('(%s; %s; %s; %s; %s; %s; %s; %s)' \\\n",
    "                                              % (criterion, min_impurity_decrease, max_features, min_samples_leaf, n_estimators, min_samples_split, max_depth, th))\n",
    "    best_score_index = np.argmax(sample_scores)\n",
    "    best_score = sample_scores[best_score_index]\n",
    "    best_params = sample_params[best_score_index]\n",
    "    if verbose:\n",
    "        print(\"\\nBEST SCORE:\\t\", best_score)\n",
    "        print(\"BEST PARAMS:\\t\", best_params)\n",
    "    return best_score, best_params, sample_scores, sample_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(useful_words_tsv, min_length=0):\n",
    "    doc_to_title = {}\n",
    "    with open(useful_words_tsv) as f:\n",
    "        for num_line, line in enumerate(f):\n",
    "            if num_line == 0:\n",
    "                continue\n",
    "            data = line.strip().split('\\t', 1)\n",
    "            doc_id = int(data[0])\n",
    "            if len(data) == 1:\n",
    "                title = ''\n",
    "            else:\n",
    "                title = data[1]\n",
    "#           магические 5 строчек!---------\n",
    "            cur = re.split(r' ',title)\n",
    "            title = ''\n",
    "            for i in cur:\n",
    "                if len(i) >= min_length:\n",
    "                    title += i + ' '\n",
    "#           ------------------------------          \n",
    "            doc_to_title[doc_id] = title\n",
    "    return doc_to_title\n",
    "\n",
    "def preprocessing_2(train_or_test_groups_csv, doc_to_title, train=True):\n",
    "    train_data = pd.read_csv(train_or_test_groups_csv)\n",
    "    traingroups_titledata = {}\n",
    "    for i in range(len(train_data)):\n",
    "        new_doc = train_data.iloc[i]\n",
    "        doc_group = new_doc['group_id']\n",
    "        doc_id = new_doc['doc_id']\n",
    "        title = doc_to_title[doc_id]\n",
    "        if doc_group not in traingroups_titledata:\n",
    "            traingroups_titledata[doc_group] = []\n",
    "        if train:\n",
    "            target = new_doc['target']\n",
    "            traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "        else:\n",
    "            traingroups_titledata[doc_group].append((doc_id, title))\n",
    "    return traingroups_titledata\n",
    "\n",
    "def preprocessing_3_old(traingroups_titledata, num_features=15, train=True):\n",
    "    y_train = []\n",
    "    X_train = []\n",
    "    groups_train = []\n",
    "    for new_group in traingroups_titledata:\n",
    "        docs = traingroups_titledata[new_group] \n",
    "        for k, tup in enumerate(docs):\n",
    "            if train:\n",
    "                doc_id, title, target_id = tup\n",
    "                y_train.append(target_id)\n",
    "            else:\n",
    "                doc_id, title = tup\n",
    "            groups_train.append(new_group)\n",
    "            all_dist = []\n",
    "            words = set(title.strip().split())\n",
    "            for j in range(0, len(docs)):\n",
    "                if k == j:\n",
    "                    continue\n",
    "                if train:\n",
    "                    doc_id_j, title_j, target_j = docs[j]\n",
    "                else:\n",
    "                    doc_id_j, title_j = docs[j]\n",
    "                words_j = set(title_j.strip().split())\n",
    "                all_dist.append(len(words.intersection(words_j)))\n",
    "            X_train.append(sorted(all_dist, reverse=True)[0:num_features])\n",
    "    if train:\n",
    "        return np.array(X_train), np.array(y_train), np.array(groups_train)\n",
    "    else:\n",
    "        return np.array(X_train), np.array([]), np.array(groups_train)\n",
    "    \n",
    "def preprocessing_3(traingroups_titledata, num_features=15, num_tfidf_features=30, train=True):\n",
    "    y = []\n",
    "    X = []\n",
    "    groups = []\n",
    "    for new_group in traingroups_titledata:\n",
    "        docs = traingroups_titledata[new_group] \n",
    "        list_data = []\n",
    "        for k, tup in enumerate(docs):\n",
    "            if train:\n",
    "                doc_id, title, target_id = tup\n",
    "                y.append(target_id)\n",
    "            else:\n",
    "                doc_id, title = tup\n",
    "            list_data.append(title)  \n",
    "            groups.append(new_group)\n",
    "\n",
    "        vectorizer = TfidfVectorizer(max_features=num_tfidf_features)\n",
    "        group_voc = vectorizer.fit_transform(list_data)\n",
    "        dist = cosine_similarity(group_voc, group_voc)\n",
    "        X_curr  = np.sort(dist, axis=1)[:,-(num_features+1):-1][:,::-1]\n",
    "        X.append(X_curr)\n",
    "    X = np.vstack(X)\n",
    "    if train:\n",
    "        return np.array(X), np.array(y), np.array(groups)\n",
    "    else:\n",
    "        return np.array(X), np.array(groups)\n",
    "    \n",
    "def preprocessing(useful_words_tsv, train_or_test_groups_csv, min_length, num_features, num_tfidf_features, \n",
    "                  train=True):\n",
    "    doc_to_title = preprocessing_1(useful_words_tsv, min_length=min_length)\n",
    "    traingroups_titledata = preprocessing_2(train_or_test_groups_csv, doc_to_title, train=train)\n",
    "    tup = preprocessing_3(traingroups_titledata, num_features=num_features, \\\n",
    "                                    num_tfidf_features=num_tfidf_features, train=train)\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 10) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 10\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_train, y_train, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features, \\\n",
    "                                              num_tfidf_features=num_tfidf_features) \n",
    "print(X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 20)"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 10\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                       min_length=min_length, num_features=num_features, \\\n",
    "                                       num_tfidf_features=num_tfidf_features) \n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 1)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/h2_mystem.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 0\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                       min_length=min_length, num_features=num_features, \\\n",
    "                                       num_tfidf_features=num_tfidf_features) \n",
    " \n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful_words_tsv = 'upload/h3_mystem.txt'\n",
    "# train_or_test_groups_csv = 'train_groups.csv'\n",
    "# min_length = 3\n",
    "# num_features = 10\n",
    "\n",
    "# X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "#                                               min_length=min_length, num_features=num_features) \n",
    "# X_train = np.hstack((X_train, X_train_addition))\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 23)"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/useful_names.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 0\n",
    "num_features = 3\n",
    "num_tfidf_features = 1000000\n",
    "\n",
    "\n",
    "X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                       min_length=min_length, num_features=num_features, \\\n",
    "                                       num_tfidf_features = num_tfidf_features) \n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(name, input_file, tr_or_tst_groups):\n",
    "    feature = pd.read_csv(input_file, sep='\\t', encoding='utf-8', lineterminator='\\n')\n",
    "    group_df = pd.read_csv(tr_or_tst_groups, sep=',', encoding='utf-8', lineterminator='\\n')\n",
    "    feature = pd.merge(feature, group_df, on='doc_id', how='right', sort=True).dropna()\n",
    "    feature['mean_'+name] = feature.groupby('group_id')[name].transform('mean')\n",
    "    feature = feature[['mean_'+name, 'pair_id',name]]\n",
    "    feature['pair_id'] = feature['pair_id'].astype(int)    \n",
    "    feature = pd.merge(group_df, feature, on='pair_id', how='left', sort=True)\n",
    "    feature['diff_'+name] = np.abs(feature['mean_'+name] - feature[name])\n",
    "\n",
    "    return np.asmatrix(feature['diff_'+name].to_numpy()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 24)"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'num_images'\n",
    "input_file = 'upload/pics_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 31)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = ''\n",
    "\n",
    "input_file = 'upload/digits_output_statistics.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "name = log+'number_cntr'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "\n",
    "input_file = 'upload/digits_output_statistics.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "name = log+'year_psbl'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "\n",
    "input_file = 'upload/digits_output_statistics.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "name = log+'near_year_psbl'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "\n",
    "input_file = 'upload/digits_output_statistics.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "name = 'useful_num_ratio'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "\n",
    "input_file = 'upload/digits_output_statistics.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "name = log+'long_numbers'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "\n",
    "input_file = 'upload/digits_output_statistics.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "name = 'long_num_ratio'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "\n",
    "input_file = 'upload/digits_output_statistics.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "name = log+'short_numbers'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ?!..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 32)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'excl_mrk_num'\n",
    "input_file = 'upload/punctuation_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 33)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'ques_mrk_num'\n",
    "input_file = 'upload/punctuation_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 34)"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'poin_mrk_num'\n",
    "input_file = 'upload/punctuation_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 35)"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'ellp_mrk_num'\n",
    "input_file = 'upload/punctuation_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 36)"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'file_size'\n",
    "input_file = 'upload/size_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "X_train_addition = do_features(name, input_file, tr_or_tst_groups)\n",
    "\n",
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'upload/lengths_output.txt'\n",
    "df_temp = pd.read_csv(input_file, sep='\\t', encoding='utf-8', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.to_csv(input_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17800"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_temp.doc_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(name, input_file, tr_or_tst_groups):\n",
    "    feature = pd.read_csv(input_file, sep='\\t', encoding='utf-8', lineterminator='\\n')\n",
    "    group_df = pd.read_csv(tr_or_tst_groups, sep=',', encoding='utf-8', lineterminator='\\n')\n",
    "    feature = pd.merge(feature, group_df, on='doc_id', how='right', sort=True).dropna()\n",
    "    print(feature.shape)\n",
    "    feature['mean_'+name] = feature.groupby('group_id')[name].transform('mean')\n",
    "    feature = feature[['mean_'+name, 'pair_id',name]]\n",
    "    feature['pair_id'] = feature['pair_id'].astype(int)    \n",
    "    feature = pd.merge(group_df, feature, on='pair_id', how='left', sort=True)\n",
    "    print(feature.shape)\n",
    "    feature['diff_'+name] = np.abs(feature['mean_'+name] - feature[name])\n",
    "#     print(feature['mean_'+name])\n",
    "    return np.asmatrix(feature['diff_'+name].to_numpy()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 doc_id\n",
      "1 addres\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-544-3b7514016081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX_train_addition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_or_tst_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     print(X_train.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_addition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "input_file = 'upload/lengths_output.txt'\n",
    "tr_or_tst_groups = 'train_groups.csv'\n",
    "df_temp = pd.read_csv(input_file, sep='\\t', encoding='utf-8', lineterminator='\\n')\n",
    "for i, n in enumerate(df_temp.columns):\n",
    "    print(i, n)\n",
    "    if i == 0 :\n",
    "        continue\n",
    "    X_train_addition = do_features(str(n), input_file, tr_or_tst_groups)\n",
    "#     print(X_train.shape)\n",
    "    X_train = np.hstack((X_train, X_train_addition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful_words_tsv = 'ngrams.txt'\n",
    "# train_or_test_groups_csv = 'train_groups.csv'\n",
    "# min_length = 0\n",
    "# num_features = 0\n",
    "\n",
    "# X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "#                                               min_length=min_length, num_features=num_features) \n",
    "# X_train = np.hstack((X_train, X_train_addition))\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Масштабирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scale = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE:  0.71241625360844\t(gini; 0.5; 5; 4; 100; 9; 10; 0.3)\n",
      "SCORE:  0.7153694009476739\t(gini; 0.5; 5; 4; 200; 9; 10; 0.3)\n",
      "SCORE:  0.7129454348696673\t(gini; 0.5; 5; 4; 300; 9; 10; 0.3)\n",
      "SCORE:  0.7149752279162866\t(gini; 0.5; 15; 4; 100; 9; 10; 0.3)\n",
      "SCORE:  0.7153119540768076\t(gini; 0.5; 15; 4; 200; 9; 10; 0.3)\n",
      "SCORE:  0.7136465882400134\t(gini; 0.5; 15; 4; 300; 9; 10; 0.3)\n",
      "\n",
      "BEST SCORE:\t 0.7153694009476739\n",
      "BEST PARAMS:\t ('gini', 0.5, 5, 4, 200, 9, 10, 0.3)\n"
     ]
    }
   ],
   "source": [
    "criterion_list = ['gini']\n",
    "min_impurity_decrease_list = [0.5]\n",
    "max_features_list = [5, 15]\n",
    "min_samples_leaf_list = [4]\n",
    "n_estimators_list = [100, 200, 300]\n",
    "min_samples_split_list = [9]\n",
    "max_depth_list = [10]\n",
    "th_list = [0.3]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(criterion_list, min_impurity_decrease_list, max_features_list, \\\n",
    "                                    min_samples_leaf_list, n_estimators_list, \\\n",
    "                                    min_samples_split_list, max_depth_list, th_list, \\\n",
    "                                    X_train_scale, y_train, groups_train, kfold_generator, \\\n",
    "                                    folds=3, repeats=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE:  0.6485250133136395\t(gini; 0.5; 5; 4; 90; 9; 10; 0.3)\n",
      "SCORE:  0.6510352034170339\t(gini; 0.5; 5; 4; 120; 9; 10; 0.3)\n",
      "SCORE:  0.6424916967400158\t(gini; 0.5; 7; 4; 90; 9; 10; 0.3)\n",
      "SCORE:  0.6483689066974057\t(gini; 0.5; 7; 4; 120; 9; 10; 0.3)\n",
      "SCORE:  0.6515282152311481\t(gini; 0.5; 9; 4; 90; 9; 10; 0.3)\n",
      "SCORE:  0.6534668821352425\t(gini; 0.5; 9; 4; 120; 9; 10; 0.3)\n",
      "\n",
      "BEST SCORE:\t 0.6534668821352425\n",
      "BEST PARAMS:\t ('gini', 0.5, 9, 4, 120, 9, 10, 0.3)\n"
     ]
    }
   ],
   "source": [
    "criterion_list = ['gini']\n",
    "min_impurity_decrease_list = [0.5]\n",
    "max_features_list = [5, 7, 9]\n",
    "min_samples_leaf_list = [4]\n",
    "n_estimators_list = [90, 120]\n",
    "min_samples_split_list = [9]\n",
    "max_depth_list = [10]\n",
    "th_list = [0.3]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(criterion_list, min_impurity_decrease_list, max_features_list, \\\n",
    "                                    min_samples_leaf_list, n_estimators_list, \\\n",
    "                                    min_samples_split_list, max_depth_list, th_list, \\\n",
    "                                    X_train_scale, y_train, groups_train, kfold_generator, \\\n",
    "                                    folds=3, repeats=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE:  0.6960519380004604\t(gini; 0.5; 7; 4; 120; 9; 10; 0.3)\n",
      "SCORE:  0.6938376377591305\t(gini; 0.5; 7; 4; 320; 9; 10; 0.3)\n",
      "SCORE:  0.6959058306501704\t(gini; 0.5; 14; 4; 120; 9; 10; 0.3)\n",
      "SCORE:  0.6926607575373621\t(gini; 0.5; 14; 4; 320; 9; 10; 0.3)\n",
      "\n",
      "BEST SCORE:\t 0.6960519380004604\n",
      "BEST PARAMS:\t ('gini', 0.5, 7, 4, 120, 9, 10, 0.3)\n"
     ]
    }
   ],
   "source": [
    "criterion_list = ['gini']\n",
    "min_impurity_decrease_list = [0.5]\n",
    "max_features_list = [7, 14]\n",
    "min_samples_leaf_list = [4]\n",
    "n_estimators_list = [120, 320]\n",
    "min_samples_split_list = [9]\n",
    "max_depth_list = [10]\n",
    "th_list = [0.3]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(criterion_list, min_impurity_decrease_list, max_features_list, \\\n",
    "                                    min_samples_leaf_list, n_estimators_list, \\\n",
    "                                    min_samples_split_list, max_depth_list, th_list, \\\n",
    "                                    X_train_scale, y_train, groups_train, kfold_generator, \\\n",
    "                                    folds=3, repeats=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE:  0.7076414291227606\t(gini; 0.5; 14; 4; 50; 9; 10; 0.3)\n",
      "SCORE:  0.7109649291094572\t(gini; 0.5; 14; 4; 160; 9; 10; 0.3)\n",
      "SCORE:  0.7139249060904231\t(gini; 0.5; 14; 4; 320; 9; 10; 0.3)\n",
      "\n",
      "BEST SCORE:\t 0.7139249060904231\n",
      "BEST PARAMS:\t ('gini', 0.5, 14, 4, 320, 9, 10, 0.3)\n"
     ]
    }
   ],
   "source": [
    "criterion_list = ['gini']\n",
    "min_impurity_decrease_list = [0.5]\n",
    "max_features_list = [14]\n",
    "min_samples_leaf_list = [4]\n",
    "n_estimators_list = [50, 160, 320]\n",
    "min_samples_split_list = [9]\n",
    "max_depth_list = [10]\n",
    "th_list = [0.3]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(criterion_list, min_impurity_decrease_list, max_features_list, \\\n",
    "                                    min_samples_leaf_list, n_estimators_list, \\\n",
    "                                    min_samples_split_list, max_depth_list, th_list, \\\n",
    "                                    X_train_scale, y_train, groups_train, kfold_generator, \\\n",
    "                                    folds=3, repeats=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=10, max_features=7, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=4, min_samples_split=9,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=120,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth=10, min_samples_split=9, n_estimators=120,\\\n",
    "                   min_samples_leaf=4, max_features=9, criterion='gini')\n",
    "model.fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.167166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.104157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0.068981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.058993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.055876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.034748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.033981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.031484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.030563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.030042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.027436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.026889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.025679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.024267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.022908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.022260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.021658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.020541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.019997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.017517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.016471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.014901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.013966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.013048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.011907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.010233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.009396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     power\n",
       "0   27.0  0.167166\n",
       "1   28.0  0.104157\n",
       "2   26.0  0.068981\n",
       "3   20.0  0.058993\n",
       "4    1.0  0.056840\n",
       "5   25.0  0.055876\n",
       "6    3.0  0.034748\n",
       "7   17.0  0.033981\n",
       "8    2.0  0.031484\n",
       "9   29.0  0.030563\n",
       "10  23.0  0.030042\n",
       "11  22.0  0.027436\n",
       "12  19.0  0.026889\n",
       "13   7.0  0.025679\n",
       "14   5.0  0.024267\n",
       "15  24.0  0.022908\n",
       "16  18.0  0.022260\n",
       "17   6.0  0.021658\n",
       "18  11.0  0.020541\n",
       "19  21.0  0.019997\n",
       "20   8.0  0.017517\n",
       "21   4.0  0.016471\n",
       "22  12.0  0.014901\n",
       "23  13.0  0.013966\n",
       "24  10.0  0.013048\n",
       "25   9.0  0.011907\n",
       "26  16.0  0.010233\n",
       "27  15.0  0.009396\n",
       "28  14.0  0.008097"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = model.feature_importances_\n",
    "f_num = np.array(range(1,len(imp)+1))\n",
    "tab = np.vstack((f_num, imp)).T\n",
    "df = pd.DataFrame(tab, columns=['id', 'power'])\n",
    "df = df.sort_values(by=['power'], ascending=False)\n",
    "df = df.reset_index()\n",
    "df = df.drop(labels=['index'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE:  0.6961779561284294\t(gini; 0.5; 14; 4; 160; 9; 10; 0.3)\n",
      "\n",
      "BEST SCORE:\t 0.6961779561284294\n",
      "BEST PARAMS:\t ('gini', 0.5, 14, 4, 160, 9, 10, 0.3)\n"
     ]
    }
   ],
   "source": [
    "criterion_list = ['gini']\n",
    "min_impurity_decrease_list = [0.5]\n",
    "max_features_list = [14]\n",
    "min_samples_leaf_list = [4]\n",
    "n_estimators_list = [160]\n",
    "min_samples_split_list = [9]\n",
    "max_depth_list = [10]\n",
    "th_list = [0.3]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(criterion_list, min_impurity_decrease_list, max_features_list, \\\n",
    "                                    min_samples_leaf_list, n_estimators_list, \\\n",
    "                                    min_samples_split_list, max_depth_list, th_list, \\\n",
    "                                    X_train_scale, y_train, groups_train, kfold_generator, \\\n",
    "                                    folds=3, repeats=32, verbose=True)\n",
    "    \n",
    "#     alpha_list, C_list, max_epoch_list, th_list, balance_ratio_list, \\\n",
    "#                                     X_train_scale, y_train, groups_train, kfold_generator, batch_generator, \\\n",
    "#                                     model_type='rforest_logreg', folds=3, repeats=32, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, test_groups_csv, out_file, target='target', index_label=\"pair_id\"):\n",
    "    indices = np.asarray(pd.read_csv(test_groups_csv)[index_label])\n",
    "    predicted_df = pd.DataFrame(predicted_labels, index = indices, columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 20)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/title_output_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 20\n",
    "\n",
    "X_test, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                          min_length=min_length, num_features=num_features, train=False) \n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 5)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'upload/useful_names.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 5\n",
    "\n",
    "X_test_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                          min_length=min_length, num_features=num_features, train=False) \n",
    "X_test = np.hstack((X_test, X_test_addition))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 27)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_addition = pd.read_csv('upload/diff_features.csv', encoding='utf-8', lineterminator='\\n')['diff']\n",
    "X_test_addition = np.array(X_test_addition.tolist(), ndmin=2).T\n",
    "\n",
    "X_test = np.hstack((X_test, X_test_addition))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words_tsv = 'upload/h1_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 10\n",
    "\n",
    "X_test_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features) \n",
    "X_test = np.hstack((X_test, X_test_addition))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words_tsv = 'upload/h2_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 10\n",
    "\n",
    "X_test_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features) \n",
    "X_test = np.hstack((X_test, X_test_addition))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_words_tsv = 'upload/h3_mystem.txt'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 10\n",
    "\n",
    "X_test_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features) \n",
    "X_test = np.hstack((X_test, X_test_addition))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_scale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-43c0aaaa9987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                    min_samples_leaf=5, max_features=7, criterion='entropy')\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# f1_score(y_train, clf_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_scale' is not defined"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth=8, min_samples_split=10, n_estimators=20,\\\n",
    "                   min_samples_leaf=5, max_features=7, criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scale, y_train)\n",
    "y_pred = model.predict_proba(X_test_scale)\n",
    "write_to_submission_file(y_pred, 'test_groups.csv', \"mod_rforest_pred_prob.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
