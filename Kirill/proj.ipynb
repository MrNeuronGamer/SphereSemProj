{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Определяю свою LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    if batch_size > X.shape[0]:\n",
    "        batch_size = X.shape[0]\n",
    "    if shuffle:\n",
    "        new_ids = np.random.permutation(X.shape[0])\n",
    "    else:\n",
    "        new_ids = np.arange(X.shape[0])\n",
    "    number_batches = X.shape[0] // batch_size\n",
    "    for i in range(number_batches):\n",
    "        indices = range(batch_size*i, batch_size*(i+1))\n",
    "        X_batch = X[new_ids[indices]]\n",
    "        y_batch = y[new_ids[indices]]\n",
    "        yield (X_batch, y_batch)\n",
    "    if len(X) % batch_size != 0:\n",
    "        indices = batch_size*(i+1)\n",
    "        X_batch = X[new_ids[indices:]]\n",
    "        y_batch = y[new_ids[indices:]]\n",
    "        yield (X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    sigm_value_x = 1/(1 + np.exp(-(x)))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):   \n",
    "    def __init__(self, batch_generator, batch_size=50, \\\n",
    "                 C=1, alpha=0.01, max_epoch=10, model_type='logreg', th=0.5):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "        \n",
    "        self.th = th\n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        self.weights = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0.\n",
    "        if self.model_type == 'linreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                a = np.dot(x, self.weights)\n",
    "                loss += (a-y)**2\n",
    "            loss /= len(y_batch)\n",
    "            loss += np.dot(self.weights[1:], self.weights[1:]) / self.C\n",
    "        elif self.model_type == 'logreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                a = sigmoid(np.dot(x, self.weights))\n",
    "                temp = a**y * (1-a)**(1-y)\n",
    "                if temp < 10**(-301):                       # наугад\n",
    "                    loss -= -1000                          # наугад\n",
    "                    continue\n",
    "                loss -= np.log2(temp)\n",
    "#                 loss -= y * np.log2(a) + (1-y) * np.log2(1-a)\n",
    "            loss /= len(y_batch)\n",
    "            loss += np.dot(self.weights[1:], self.weights[1:]) / self.C\n",
    "        return loss\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"      \n",
    "        loss_grad = 0.\n",
    "        if self.model_type == 'linreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                a = np.dot(x, self.weights)\n",
    "                loss_grad += (a-y)*x\n",
    "            loss_grad /= len(y_batch)\n",
    "            R = self.weights / self.C\n",
    "            R[0] = 0\n",
    "            loss_grad += R\n",
    "        elif self.model_type == 'logreg':\n",
    "            for x, y in zip(X_batch, y_batch):\n",
    "                dot = np.dot(x, self.weights)\n",
    "                a = sigmoid(dot)\n",
    "                loss_grad += (a-y)*x\n",
    "            loss_grad /= len(y_batch)\n",
    "            R = self.weights / self.C\n",
    "            R[0] = 0\n",
    "            loss_grad += R\n",
    "        return loss_grad\n",
    "    \n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"    \n",
    "        alpha_k = self.alpha / self.curr_epoch**(0.005)\n",
    "        self.weights = self.weights - alpha_k * new_grad\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''    \n",
    "        if self.model_type == 'linreg':\n",
    "            y = y - np.mean(y)\n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        self.weights = X[np.random.randint(0, X.shape[0]-1)]\n",
    "        self.curr_epoch = 0\n",
    "        for n in range(0, self.max_epoch):\n",
    "            self.curr_epoch += 1\n",
    "            new_epoch_generator = self.batch_generator(X, y, shuffle=True, batch_size=self.batch_size)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)  \n",
    "        return self\n",
    "     \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''        \n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        y_hat = np.array([])        \n",
    "        if self.model_type == 'linreg':\n",
    "            y_hat = np.dot(X, self.weights) / np.sum(X)\n",
    "        elif self.model_type == 'logreg':\n",
    "            dot_func = lambda x: sigmoid(np.dot(x, self.weights))\n",
    "            y_hat = np.apply_along_axis(dot_func, 1, X)\n",
    "        y_hat = np.vstack((1-y_hat, y_hat)).T\n",
    "        return y_hat\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''        \n",
    "        y_hat = self.predict_proba(X)\n",
    "        if self.model_type == 'logreg':\n",
    "            y_hat = y_hat - self.th > 0\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class rforest_plus_logreg(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, SGD_mod, RForest_mod, th=0.5):\n",
    "        self.SGD_mod = SGD_mod\n",
    "        self.RForest_mod = RForest_mod\n",
    "        self.th = th\n",
    "    def fit(self, X, y):\n",
    "        self.SGD_mod.fit(X,y)\n",
    "        self.RForest_mod.fit(X,y)\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        y_pred = self.SGD_mod.predict_proba(X)[:,1]\n",
    "        y_pred = np.vstack((y_pred, self.RForest_mod.predict_proba(X)[:,1]))\n",
    "        y_pred = np.mean(y_pred, axis=0)   \n",
    "        y_pred = np.vstack((1-y_pred, y_pred)).T\n",
    "        return y_pred\n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_proba(X)[:,1]\n",
    "        return (y_pred - self.th > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_generator(groups_train, n_splits=10):\n",
    "    all_groups = np.unique(groups_train)\n",
    "    fold_size = len(all_groups) // n_splits\n",
    "    all_groups = np.random.permutation(all_groups)\n",
    "    fold_groups = np.zeros((n_splits,fold_size), dtype=int)\n",
    "    for i, group in enumerate(all_groups):\n",
    "        fold = i // fold_size\n",
    "        if fold == n_splits:\n",
    "            break\n",
    "        group_i = i % fold_size\n",
    "        fold_groups[fold,group_i] = group\n",
    "    fold_indices = {}\n",
    "    for fold in range(n_splits):\n",
    "        indices = np.array([], dtype = int)\n",
    "        for group in fold_groups[fold]:\n",
    "            indices = np.append(indices, np.argwhere(groups_train == group))\n",
    "        fold_indices[fold] = indices\n",
    "\n",
    "    for i in fold_indices:\n",
    "        kf_test = fold_indices[i]\n",
    "        kf_train = np.array([],dtype=int)\n",
    "        for j in fold_indices:\n",
    "            if i == j:\n",
    "                continue\n",
    "            kf_train = np.append(kf_train, fold_indices[j])\n",
    "        kf_tuple = [kf_train, kf_test]\n",
    "        yield (kf_train, kf_test)\n",
    "        \n",
    "def cross_validation(model, groups_train, kfold_generator, X, y, \\\n",
    "                     folds=10, th=0.5, verbose=False):    \n",
    "    total_score = 0\n",
    "    for i, tuple_indices in enumerate(kfold_generator(groups_train, n_splits=folds)):\n",
    "        train_index, test_index = tuple_indices\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_test)[:,1]  \n",
    "            \n",
    "        score = f1_score(y_test, (y_pred - th > 0))\n",
    "        total_score += score\n",
    "        if verbose:\n",
    "            print(i, \"score:\", score)\n",
    "    mean_score = total_score / folds\n",
    "    if verbose:\n",
    "        print(\"MEAN_SCORE:\", mean_score)\n",
    "    return mean_score\n",
    "\n",
    "def grid_cv(alpha_list, C_list, max_epoch_list, th_list, \n",
    "            X, y, groups_train, kfold_generator, batch_generator, \n",
    "            model_type='logreg', folds=10, repeats=1, verbose=True):\n",
    "    sample_scores = np.array([])\n",
    "    sample_params = []\n",
    "    for alpha in alpha_list:\n",
    "        for C in C_list:\n",
    "            for max_epoch in max_epoch_list:\n",
    "                for th in th_list: \n",
    "                    curr_mean_score_list = np.array([])\n",
    "                    for r in range(repeats):\n",
    "                        if model_type == 'rforest_logreg' or model_type == 'logreg_rforest':\n",
    "                            RForest_mod = RandomForestClassifier(max_depth=8, min_samples_split=10, \\\n",
    "                                                                 n_estimators=20, min_samples_leaf=5, \\\n",
    "                                                                 max_features=7, criterion='entropy')\n",
    "                            SGD_mod = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                                                      alpha=alpha, C=C, max_epoch=max_epoch)\n",
    "                            model = rforest_plus_logreg(SGD_mod, RForest_mod, th=th)\n",
    "                        elif model_type == 'logreg' or model_type == 'linreg':\n",
    "                            model = MySGDClassifier(batch_generator=batch_generator, model_type=model_type, \\\n",
    "                                                      alpha=alpha, C=C, max_epoch=max_epoch, th=th)\n",
    "                        elif model_type == 'rforest':\n",
    "                            model = RandomForestClassifier(max_depth=8, min_samples_split=10, \\\n",
    "                                                                 n_estimators=20, min_samples_leaf=5, \\\n",
    "                                                                 max_features=7, criterion='entropy')\n",
    "                            \n",
    "                        curr_score = cross_validation(model, groups_train, kfold_generator, \\\n",
    "                                                      X, y, folds=folds, th=th)\n",
    "                        curr_mean_score_list = np.append(curr_mean_score_list, curr_score)\n",
    "                    curr_mean_score = curr_mean_score_list.mean()\n",
    "                    sample_scores = np.append(sample_scores, curr_mean_score)\n",
    "                    sample_tuple = (alpha, C, th, max_epoch)\n",
    "                    sample_params.append(sample_tuple)\n",
    "                    if verbose:\n",
    "                        print(\"SCORE: \", curr_mean_score, end='\\t\\t')\n",
    "                        print(\"(alpha = %s; C = %s; max_epoch = %s; th = %s)\" % (alpha, C, max_epoch, th))\n",
    "    best_score_index = np.argmax(sample_scores)\n",
    "    best_score = sample_scores[best_score_index]\n",
    "    best_params = sample_params[best_score_index]\n",
    "    if verbose:\n",
    "        print(\"\\nBEST SCORE:\\t\", best_score)\n",
    "        print(\"BEST PARAMS:\\t\", best_params)\n",
    "    return best_score, best_params, sample_scores, sample_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просто переписал код из второй домашки в виде функций\n",
    "import re\n",
    "\n",
    "def preprocessing_1(useful_words_tsv, min_length=0):\n",
    "    doc_to_title = {}\n",
    "    with open(useful_words_tsv) as f:\n",
    "        for num_line, line in enumerate(f):\n",
    "            if num_line == 0:\n",
    "                continue\n",
    "            data = line.strip().split('\\t', 1)\n",
    "            doc_id = int(data[0])\n",
    "            if len(data) == 1:\n",
    "                title = ''\n",
    "            else:\n",
    "                title = data[1]\n",
    "#           магические 5 строчек!---------\n",
    "            cur = re.split(r' ',title)\n",
    "            title = ''\n",
    "            for i in cur:\n",
    "                if len(i) >= min_length:\n",
    "                    title += i + ' '\n",
    "#           ------------------------------          \n",
    "            doc_to_title[doc_id] = title\n",
    "    return doc_to_title\n",
    "\n",
    "def preprocessing_2(train_or_test_groups_csv, doc_to_title, train=True):\n",
    "    train_data = pd.read_csv(train_or_test_groups_csv)\n",
    "    traingroups_titledata = {}\n",
    "    for i in range(len(train_data)):\n",
    "        new_doc = train_data.iloc[i]\n",
    "        doc_group = new_doc['group_id']\n",
    "        doc_id = new_doc['doc_id']\n",
    "        title = doc_to_title[doc_id]\n",
    "        if doc_group not in traingroups_titledata:\n",
    "            traingroups_titledata[doc_group] = []\n",
    "        if train:\n",
    "            target = new_doc['target']\n",
    "            traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "        else:\n",
    "            traingroups_titledata[doc_group].append((doc_id, title))\n",
    "    return traingroups_titledata\n",
    "\n",
    "def preprocessing_3(traingroups_titledata, num_features=15, train=True):\n",
    "    y_train = []\n",
    "    X_train = []\n",
    "    groups_train = []\n",
    "    for new_group in traingroups_titledata:\n",
    "        docs = traingroups_titledata[new_group] \n",
    "        for k, tup in enumerate(docs):\n",
    "            if train:\n",
    "                doc_id, title, target_id = tup\n",
    "                y_train.append(target_id)\n",
    "            else:\n",
    "                doc_id, title = tup\n",
    "            groups_train.append(new_group)\n",
    "            all_dist = []\n",
    "            words = set(title.strip().split())\n",
    "            for j in range(0, len(docs)):\n",
    "                if k == j:\n",
    "                    continue\n",
    "                if train:\n",
    "                    doc_id_j, title_j, target_j = docs[j]\n",
    "                else:\n",
    "                    doc_id_j, title_j = docs[j]\n",
    "                words_j = set(title_j.strip().split())\n",
    "                all_dist.append(len(words.intersection(words_j)))\n",
    "            X_train.append(sorted(all_dist, reverse=True)[0:num_features])\n",
    "    if train:\n",
    "        return np.array(X_train), np.array(y_train), np.array(groups_train)\n",
    "    else:\n",
    "        return np.array(X_train), np.array([]), np.array(groups_train)\n",
    "    \n",
    "def preprocessing(useful_words_tsv, train_or_test_groups_csv, min_length, num_features, train=True):\n",
    "    doc_to_title = preprocessing_1(useful_words_tsv, min_length=min_length)\n",
    "    traingroups_titledata = preprocessing_2(train_or_test_groups_csv, doc_to_title, train=train)\n",
    "    X_train, y_train, groups_train = preprocessing_3(traingroups_titledata, num_features=num_features, train=train)\n",
    "    return X_train, y_train, groups_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Старые фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 20) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'docs_titles.tsv'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 20\n",
    "\n",
    "X_train, y_train, groups_train = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features) \n",
    "print(X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пересечение слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 1)\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2  # $ pip install pymorphy2\n",
    "  \n",
    "def pos(word, morth=pymorphy2.MorphAnalyzer()):\n",
    "    \"Return a likely part of speech for the *word*.\"\"\"\n",
    "    return morth.parse(word)[0].tag.POS\n",
    "  \n",
    "words = \"Однако я так и не смог закончить\".split()\n",
    "functors_pos = {'INTJ', 'PRCL', 'CONJ', 'PREP'} \n",
    "\n",
    "import re\n",
    "doc_to_title = {}\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        r = [ i.lower() for i in re.split(r'\\W+',title) if i]\n",
    "        p = [re.sub(r'[^А-я]', '', i) for i in r]\n",
    "        p1 = list(filter(lambda i: i, p))\n",
    "        cur = [word for word in p1 if (pos(word) not in functors_pos and len(word)>1)]\n",
    "        cur = [morph.parse(o)[0].normal_form for o in cur]\n",
    "        title = ''\n",
    "        for i in cur:\n",
    "            if len(i)>2:\n",
    "                title += i + ' '        \n",
    "        doc_to_title[doc_id] = title\n",
    "        \n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "# y_train = []\n",
    "X_train_addition = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "#         y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        sum1 = 0\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            y = len(words.intersection(words_j))\n",
    "            all_dist.append(y)\n",
    "            sum1+=y\n",
    "        X_train_addition.append([sum1])\n",
    "X_train_addition = np.array(X_train_addition)\n",
    "# y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print(X_train_addition.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 21)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.hstack((X_train, X_train_addition))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Признаки из url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 2)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'useful_names.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 0\n",
    "num_features = 2\n",
    "\n",
    "X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features) \n",
    "print(X_train_addition.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 23)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = np.hstack((X_train, X_train_addition))\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dif_mean_pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 24)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_addition = pd.read_csv('diff_features.csv', encoding='utf-8', lineterminator='\\n')['diff']\n",
    "X_train_addition = np.array(X_train_addition.tolist(), ndmin=2).T\n",
    "\n",
    "X_train_new = np.hstack((X_train_new, X_train_addition))\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 0)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'ngrams.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 0\n",
    "num_features = 0\n",
    "\n",
    "X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features) \n",
    "print(X_train_addition.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 24)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = np.hstack((X_train_new, X_train_addition))\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 0)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'ngrams_sitenames.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 0\n",
    "num_features = 0\n",
    "\n",
    "X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features) \n",
    "print(X_train_addition.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 24)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = np.hstack((X_train_new, X_train_addition))\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 10)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'h1.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 10\n",
    "\n",
    "X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features) \n",
    "print(X_train_addition.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 34)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = np.hstack((X_train_new, X_train_addition))\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11690, 44)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'h2.txt'\n",
    "train_or_test_groups_csv = 'train_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 10\n",
    "\n",
    "X_train_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                                              min_length=min_length, num_features=num_features) \n",
    "print(X_train_addition.shape)\n",
    "\n",
    "X_train_new = np.hstack((X_train_new, X_train_addition))\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Масштабирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_new)\n",
    "X_train_scale = scaler.transform(X_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE:  0.7005249029550937\t\t(alpha = 0.1; C = 500; max_epoch = 8; th = 0.26)\n",
      "\n",
      "BEST SCORE:\t 0.7005249029550937\n",
      "BEST PARAMS:\t (0.1, 500, 0.26, 8)\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.1]\n",
    "C_list = [500]\n",
    "max_epoch_list = [8]\n",
    "th_list = [0.26]\n",
    "\n",
    "best_score, _, sample_scores, _ = grid_cv(alpha_list, C_list, max_epoch_list, th_list, \n",
    "                                    X_train_scale, y_train, groups_train, kfold_generator, batch_generator, \n",
    "                                    model_type='rforest', folds=3, repeats=16, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, test_groups_csv, out_file, target='target', index_label=\"pair_id\"):\n",
    "    indices = np.asarray(pd.read_csv(test_groups_csv)[index_label])\n",
    "    predicted_df = pd.DataFrame(predicted_labels, index = indices, columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 20)\n"
     ]
    }
   ],
   "source": [
    "useful_words_tsv = 'docs_titles.tsv'\n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 3\n",
    "num_features = 20\n",
    "\n",
    "X_test, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                          min_length=min_length, num_features=num_features, train=False) \n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 21)"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2  # $ pip install pymorphy2\n",
    "  \n",
    "def pos(word, morth=pymorphy2.MorphAnalyzer()):\n",
    "    \"Return a likely part of speech for the *word*.\"\"\"\n",
    "    return morth.parse(word)[0].tag.POS\n",
    "  \n",
    "words = \"Однако я так и не смог закончить\".split()\n",
    "functors_pos = {'INTJ', 'PRCL', 'CONJ', 'PREP'} \n",
    "\n",
    "\n",
    "import re\n",
    "doc_to_title = {}\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        r = [ i.lower() for i in re.split(r'\\W+',title) if i]\n",
    "        p = [re.sub(r'[^А-я]', '', i) for i in r]\n",
    "        p1 = list(filter(lambda i: i, p))\n",
    "        cur = [word for word in p1 if (pos(word) not in functors_pos and len(word)>1)]\n",
    "        cur = [morph.parse(o)[0].normal_form for o in cur]\n",
    "        title = ''\n",
    "        for i in cur:\n",
    "            if len(i)>2:\n",
    "                title += i + ' '\n",
    "#         print(title)\n",
    "        \n",
    "        doc_to_title[doc_id] = title\n",
    "# print (len(doc_to_title))\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "test_data = pd.read_csv('test_groups.csv')\n",
    "test_groups_titledata = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc1 = test_data.iloc[i]\n",
    "    doc_group1 = new_doc1['group_id']\n",
    "    doc_id1 = new_doc1['doc_id']\n",
    "    title1 = doc_to_title[doc_id1]\n",
    "    if doc_group1 not in test_groups_titledata:\n",
    "        test_groups_titledata[doc_group1] = []\n",
    "    test_groups_titledata[doc_group1].append((doc_id1, title1))\n",
    "X_test_addition = []\n",
    "groups_test = []\n",
    "for new_group in test_groups_titledata:\n",
    "    docs1 = test_groups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs1):\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        sum1 = 0\n",
    "        for j in range(0, len(docs1)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j= docs1[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            y = len(words.intersection(words_j))\n",
    "            all_dist.append(y)\n",
    "            sum1+=y\n",
    "#             all_dist.append(len(words.intersection(words_j)))\n",
    "        X_test_addition.append([sum1])\n",
    "X_test_addition = np.array(X_test_addition)\n",
    "groups_test = np.array(groups_test)\n",
    "\n",
    "X_test = np.hstack((X_test, X_test_addition))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 26)"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_words_tsv = 'useful_names.txt' \n",
    "train_or_test_groups_csv = 'test_groups.csv'\n",
    "min_length = 1\n",
    "num_features = 5\n",
    "\n",
    "X_test_addition, _, _ = preprocessing(useful_words_tsv, train_or_test_groups_csv, \\\n",
    "                          min_length=min_length, num_features=num_features, train=False) \n",
    "X_test = np.hstack((X_test, X_test_addition))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 27)"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_addition = pd.read_csv('diff_features_test.csv', encoding='utf-8', lineterminator='\\n')['diff']\n",
    "X_test_addition = np.array(X_test_addition.tolist(), ndmin=2).T\n",
    "\n",
    "X_test = np.hstack((X_test, X_test_addition))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "#                             alpha=0.1, C=500, th=0.26, max_epoch=10) \n",
    "# log_reg.fit(X_train_scale, y_train)\n",
    "# y_pred = log_reg.predict(X_test_scale)[:,1].astype(int)\n",
    "# write_to_submission_file(y_pred, 'test_groups.csv', \"y_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = MySGDClassifier(batch_generator=batch_generator, model_type='logreg', \\\n",
    "                            alpha=0.1, C=500, th=0.26, max_epoch=10)\n",
    "rforest = RandomForestClassifier(max_depth=8, min_samples_split=10, \\\n",
    "                               n_estimators=20, min_samples_leaf=5, \\\n",
    "                               max_features=7, criterion='entropy')\n",
    "model = rforest_plus_logreg(log_reg, rforest, th=0.26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scale, y_train)\n",
    "y_pred = model.predict(X_test_scale)\n",
    "write_to_submission_file(y_pred, 'test_groups.csv', \"y_pred.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
